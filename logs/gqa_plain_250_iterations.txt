====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config, layer_idx: int):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        
        # flags from config
        self.use_mdha = getattr(config, "use_mdha", False)
        self.use_gqa  = getattr(config, "use_gqa", False)
        self.num_gqa_layers = getattr(config, "num_gqa_layers", 0)

        self.is_gqa_layer = (
            self.use_gqa and
            self.num_gqa_layers > 0 and
            layer_idx >= config.n_layer - self.num_gqa_layers
        )

        # K/V groups: in lower layers, num_kv_groups == n_head (baseline),
        # in upper GQA layers, num_kv_groups < n_head (e.g., 4 groups for 16 heads).
        self.num_kv_groups = (
            getattr(config, "num_kv_groups", self.n_head)
            if self.is_gqa_layer else
            self.n_head
        )

        # Q keeps full heads always
        self.c_q = nn.Linear(self.n_embd, self.n_head * self.head_dim, bias=False)
        # K/V possibly reduced to num_kv_groups
        self.c_k = nn.Linear(self.n_embd, self.num_kv_groups * self.head_dim, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.num_kv_groups * self.head_dim, bias=False)

        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_()  # zero init suggested by @Grad62304977

        self.rotary = Rotary(self.head_dim)

        # MDHA convs: depthwise-ish Conv1d along time
        if self.use_mdha:
            ksize = getattr(config, "mdha_kernel_size", 3)

            # For Q: conv over [B, H*D, T], grouped by head
            self.q_conv = nn.Conv1d(
                in_channels=self.n_head * self.head_dim,
                out_channels=self.n_head * self.head_dim,
                kernel_size=ksize,
                groups=self.n_head,
                padding=ksize - 1,
            )

            # For K/V: conv over [B, G*D, T], grouped by KV group
            kv_channels = self.num_kv_groups * self.head_dim
            self.kv_conv = nn.Conv1d(
                in_channels=kv_channels,
                out_channels=kv_channels,
                kernel_size=ksize,
                groups=self.num_kv_groups,
                padding=ksize - 1,
            )


    def forward(self, x):
        B, T, C = x.size()

        # 1) Project Q/K/V with possibly different numbers of K/V groups
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.num_kv_groups, self.head_dim)
        v = self.c_v(x).view(B, T, self.num_kv_groups, self.head_dim)

        # 2) Optional MDHA (Conv1d along time) on Q/K/V
        if self.use_mdha:
            # Q: [B, T, H, D] -> [B, H*D, T] -> conv -> back
            q_flat = q.reshape(B, T, self.n_head * self.head_dim).transpose(1, 2)
            q_flat = self.q_conv(q_flat)[..., :T]  # crop to keep sequence length T
            q = q_flat.transpose(1, 2).reshape(B, T, self.n_head, self.head_dim)

            # K/V: [B, T, G, D] -> [B, G*D, T] -> conv -> back
            kv_channels = self.num_kv_groups * self.head_dim
            k_flat = k.reshape(B, T, kv_channels).transpose(1, 2)
            v_flat = v.reshape(B, T, kv_channels).transpose(1, 2)
            k_flat = self.kv_conv(k_flat)[..., :T]
            v_flat = self.kv_conv(v_flat)[..., :T]
            k = k_flat.transpose(1, 2).reshape(B, T, self.num_kv_groups, self.head_dim)
            v = v_flat.transpose(1, 2).reshape(B, T, self.num_kv_groups, self.head_dim)

        # 3) Rotary embeddings + QK norm (same as baseline, but K has G groups instead of H)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),))

        # 4) Prepare for scaled dot-product attention
        # q_attn: [B, H, T, D]
        q_attn = q.transpose(1, 2)

        # k/v: either baseline (num_kv_groups == n_head) or GQA (num_kv_groups < n_head)
        if self.is_gqa_layer:
            assert self.n_head % self.num_kv_groups == 0, \
        f"n_head ({self.n_head}) must be divisible by num_kv_groups ({self.num_kv_groups}) in GQA layers"

            # GQA: repeat each KV group across its group of heads
            group_size = self.n_head // self.num_kv_groups
            k_group = k.transpose(1, 2)  # [B, G, T, D]
            v_group = v.transpose(1, 2)  # [B, G, T, D]
            k_attn = k_group.repeat_interleave(group_size, dim=1)
            v_attn = v_group.repeat_interleave(group_size, dim=1)
        else:
            # baseline: num_kv_groups == n_head
            k_attn = k.transpose(1, 2)
            v_attn = v.transpose(1, 2)

        # 5) Scaled dot-product attention (unchanged interface)
        y = F.scaled_dot_product_attention(
            q_attn, k_attn, v_attn,
            is_causal=True,
        )
        # 6) Re-assemble heads and project out
        y = y.transpose(1, 2).contiguous().view_as(x)
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config, layer_idx):
        super().__init__()
        self.layer_idx = layer_idx
        self.attn = CausalSelfAttention(config, layer_idx=layer_idx)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    
    # attention experiment flags
    use_mdha : bool = False
    use_gqa : bool = False
    mdha_kernel_size : int = 3
    num_kv_groups : int = 1
    num_gqa_layers : int = 0


class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config, layer_idx=i) for i in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float()
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :])
            logits = logits.float()
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 250 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end

    # attention experiment flags (wired to env vars for CLI control)
    use_mdha : bool = bool(int(os.getenv("USE_MDHA", "0")))
    use_gqa : bool = bool(int(os.getenv("USE_GQA", "0")))
    mdha_kernel_size : int = int(os.getenv("MDHA_KERNEL_SIZE", "3"))
    num_kv_groups : int = int(os.getenv("NUM_KV_GROUPS", "1"))
    num_gqa_layers : int = int(os.getenv("NUM_GQA_LAYERS", "0"))

args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(
    vocab_size=num_vocab,
    n_layer=12,
    n_head=6,
    n_embd=768,
    use_mdha=args.use_mdha,
    use_gqa=args.use_gqa,
    mdha_kernel_size=args.mdha_kernel_size,
    num_kv_groups=args.num_kv_groups,
    num_gqa_layers=args.num_gqa_layers,
))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
if master_process:
    print(f"use_mdha={args.use_mdha}, use_gqa={args.use_gqa}, "
          f"mdha_kernel_size={args.mdha_kernel_size}, "
          f"num_kv_groups={args.num_kv_groups}, num_gqa_layers={args.num_gqa_layers}")
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
# optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
#                                weight_decay=args.weight_decay, fused=True)
# optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
# optimizers = [optimizer1, optimizer2]
# collect transformer.h params into Muon-safe (2D) and the rest
muon_params = []
extra_adam_params = []

for name, p in raw_model.transformer.h.named_parameters():
    if p.ndim == 2:
        muon_params.append(p)
    else:
        extra_adam_params.append(p)

# AdamW: lm_head + any non-2D transformer params (e.g., conv weights, biases)
optimizer1 = torch.optim.AdamW(
    list(raw_model.lm_head.parameters()) + extra_adam_params,
    lr=args.learning_rate,
    betas=(0.9, 0.95),
    weight_decay=args.weight_decay,
    fused=True,
)

# Muon: only 2D parameters in the transformer blocks
optimizer2 = Muon(muon_params, lr=0.1*args.learning_rate, momentum=0.95)

optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

        

        f.write("Experiment config:\n")
        f.write(f"  use_mdha       = {args.use_mdha}\n")
        f.write(f"  mdha_kernel_sz = {args.mdha_kernel_size}\n")
        f.write(f"  use_gqa        = {args.use_gqa}\n")
        f.write(f"  num_kv_groups  = {args.num_kv_groups}\n")
        f.write(f"  num_gqa_layers = {args.num_gqa_layers}\n")
        f.write('='*100 + '\n')


training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            _, loss = model(x, y, return_logits=False)
            train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Mon Dec  8 17:34:38 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:90:00.0 Off |                    0 |
| N/A   47C    P0             93W /  400W |    2525MiB /  81920MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:B7:00.0 Off |                    0 |
| N/A   46C    P0             89W /  400W |    2525MiB /  81920MiB |     14%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Experiment config:
  use_mdha       = False
  mdha_kernel_sz = 3
  use_gqa        = True
  num_kv_groups  = 3
  num_gqa_layers = 4
====================================================================================================
step:0/250 val_loss:15.9718 train_time:271ms step_avg:nanms
step:1/250 train_loss:15.9741 train_time:96108ms step_avg:nanms
step:2/250 train_loss:10.6745 train_time:97425ms step_avg:nanms
step:3/250 train_loss:9.4857 train_time:98733ms step_avg:nanms
step:4/250 train_loss:8.8948 train_time:100024ms step_avg:nanms
step:5/250 train_loss:8.6626 train_time:101307ms step_avg:nanms
step:6/250 train_loss:8.3063 train_time:102589ms step_avg:nanms
step:7/250 train_loss:8.1553 train_time:103884ms step_avg:nanms
step:8/250 train_loss:7.7189 train_time:105187ms step_avg:nanms
step:9/250 train_loss:7.8034 train_time:106470ms step_avg:nanms
step:10/250 train_loss:7.5480 train_time:107785ms step_avg:nanms
step:11/250 train_loss:7.4335 train_time:1302ms step_avg:nanms
step:12/250 train_loss:7.3022 train_time:2645ms step_avg:nanms
step:13/250 train_loss:7.2521 train_time:3953ms step_avg:1317.76ms
step:14/250 train_loss:7.1614 train_time:5256ms step_avg:1313.88ms
step:15/250 train_loss:7.2132 train_time:6551ms step_avg:1310.11ms
step:16/250 train_loss:7.0248 train_time:7846ms step_avg:1307.65ms
step:17/250 train_loss:6.9904 train_time:9141ms step_avg:1305.91ms
step:18/250 train_loss:7.4328 train_time:10466ms step_avg:1308.22ms
step:19/250 train_loss:6.9866 train_time:11790ms step_avg:1310.01ms
step:20/250 train_loss:7.1280 train_time:13095ms step_avg:1309.49ms
step:21/250 train_loss:6.9145 train_time:14389ms step_avg:1308.10ms
step:22/250 train_loss:6.8019 train_time:15725ms step_avg:1310.45ms
step:23/250 train_loss:6.8151 train_time:17104ms step_avg:1315.67ms
step:24/250 train_loss:6.8026 train_time:18403ms step_avg:1314.49ms
step:25/250 train_loss:6.5930 train_time:19761ms step_avg:1317.40ms
step:26/250 train_loss:6.7933 train_time:21060ms step_avg:1316.26ms
step:27/250 train_loss:6.7077 train_time:22383ms step_avg:1316.64ms
step:28/250 train_loss:6.7153 train_time:23743ms step_avg:1319.05ms
step:29/250 train_loss:6.6596 train_time:25058ms step_avg:1318.83ms
step:30/250 train_loss:6.7371 train_time:26367ms step_avg:1318.36ms
step:31/250 train_loss:7.1178 train_time:27661ms step_avg:1317.20ms
step:32/250 train_loss:6.5584 train_time:28965ms step_avg:1316.60ms
step:33/250 train_loss:6.4647 train_time:30291ms step_avg:1316.99ms
step:34/250 train_loss:6.4991 train_time:31609ms step_avg:1317.03ms
step:35/250 train_loss:6.6742 train_time:32911ms step_avg:1316.45ms
step:36/250 train_loss:6.6630 train_time:34252ms step_avg:1317.38ms
step:37/250 train_loss:6.6033 train_time:35583ms step_avg:1317.88ms
step:38/250 train_loss:6.4727 train_time:36914ms step_avg:1318.35ms
step:39/250 train_loss:6.5736 train_time:38224ms step_avg:1318.06ms
step:40/250 train_loss:6.3796 train_time:39544ms step_avg:1318.15ms
step:41/250 train_loss:6.5348 train_time:40880ms step_avg:1318.70ms
step:42/250 train_loss:6.4512 train_time:42198ms step_avg:1318.68ms
step:43/250 train_loss:6.4353 train_time:43531ms step_avg:1319.11ms
step:44/250 train_loss:6.3881 train_time:44856ms step_avg:1319.29ms
step:45/250 train_loss:6.2883 train_time:46203ms step_avg:1320.10ms
step:46/250 train_loss:6.3547 train_time:47527ms step_avg:1320.20ms
step:47/250 train_loss:6.2801 train_time:48848ms step_avg:1320.21ms
step:48/250 train_loss:6.4501 train_time:50175ms step_avg:1320.39ms
step:49/250 train_loss:6.1981 train_time:51501ms step_avg:1320.54ms
step:50/250 train_loss:6.3569 train_time:52828ms step_avg:1320.71ms
step:51/250 train_loss:6.4029 train_time:54157ms step_avg:1320.91ms
step:52/250 train_loss:6.4059 train_time:55480ms step_avg:1320.96ms
step:53/250 train_loss:6.3328 train_time:56810ms step_avg:1321.17ms
step:54/250 train_loss:6.3492 train_time:58113ms step_avg:1320.75ms
step:55/250 train_loss:6.2447 train_time:59424ms step_avg:1320.54ms
step:56/250 train_loss:6.2201 train_time:60750ms step_avg:1320.66ms
step:57/250 train_loss:6.2568 train_time:62047ms step_avg:1320.14ms
step:58/250 train_loss:6.2724 train_time:63345ms step_avg:1319.69ms
step:59/250 train_loss:6.3450 train_time:64647ms step_avg:1319.32ms
step:60/250 train_loss:6.1952 train_time:65951ms step_avg:1319.03ms
step:61/250 train_loss:6.2732 train_time:67258ms step_avg:1318.79ms
step:62/250 train_loss:6.3058 train_time:68569ms step_avg:1318.64ms
step:63/250 train_loss:6.2341 train_time:69909ms step_avg:1319.03ms
step:64/250 train_loss:6.1985 train_time:71235ms step_avg:1319.16ms
step:65/250 train_loss:6.0272 train_time:72558ms step_avg:1319.23ms
step:66/250 train_loss:6.0458 train_time:73872ms step_avg:1319.14ms
step:67/250 train_loss:6.2596 train_time:75217ms step_avg:1319.59ms
step:68/250 train_loss:6.2263 train_time:76561ms step_avg:1320.01ms
step:69/250 train_loss:6.2717 train_time:77899ms step_avg:1320.32ms
step:70/250 train_loss:6.0960 train_time:79205ms step_avg:1320.09ms
step:71/250 train_loss:6.2774 train_time:80526ms step_avg:1320.09ms
step:72/250 train_loss:6.1889 train_time:81870ms step_avg:1320.49ms
step:73/250 train_loss:6.2293 train_time:83179ms step_avg:1320.30ms
step:74/250 train_loss:5.9467 train_time:84504ms step_avg:1320.38ms
step:75/250 train_loss:6.0451 train_time:85829ms step_avg:1320.45ms
step:76/250 train_loss:6.0214 train_time:87155ms step_avg:1320.53ms
step:77/250 train_loss:6.1767 train_time:88459ms step_avg:1320.28ms
step:78/250 train_loss:6.0758 train_time:89770ms step_avg:1320.15ms
step:79/250 train_loss:5.6479 train_time:91068ms step_avg:1319.83ms
step:80/250 train_loss:6.0887 train_time:92385ms step_avg:1319.79ms
step:81/250 train_loss:6.1173 train_time:93700ms step_avg:1319.72ms
step:82/250 train_loss:6.0375 train_time:95012ms step_avg:1319.61ms
step:83/250 train_loss:6.1299 train_time:96320ms step_avg:1319.45ms
step:84/250 train_loss:6.0137 train_time:97626ms step_avg:1319.27ms
step:85/250 train_loss:6.0912 train_time:98957ms step_avg:1319.42ms
step:86/250 train_loss:6.0361 train_time:100267ms step_avg:1319.30ms
step:87/250 train_loss:6.1200 train_time:101583ms step_avg:1319.26ms
step:88/250 train_loss:5.9524 train_time:102882ms step_avg:1319.00ms
step:89/250 train_loss:5.9393 train_time:104203ms step_avg:1319.03ms
step:90/250 train_loss:5.7813 train_time:105503ms step_avg:1318.78ms
step:91/250 train_loss:6.0365 train_time:106804ms step_avg:1318.57ms
step:92/250 train_loss:5.9840 train_time:108110ms step_avg:1318.42ms
step:93/250 train_loss:6.2382 train_time:109426ms step_avg:1318.39ms
step:94/250 train_loss:6.1091 train_time:110746ms step_avg:1318.41ms
step:95/250 train_loss:5.8678 train_time:112075ms step_avg:1318.53ms
step:96/250 train_loss:5.9485 train_time:113376ms step_avg:1318.32ms
step:97/250 train_loss:6.0610 train_time:114675ms step_avg:1318.10ms
step:98/250 train_loss:5.8518 train_time:115976ms step_avg:1317.91ms
step:99/250 train_loss:5.8411 train_time:117350ms step_avg:1318.54ms
step:100/250 train_loss:5.8662 train_time:118654ms step_avg:1318.38ms
step:101/250 train_loss:5.7286 train_time:119979ms step_avg:1318.45ms
step:102/250 train_loss:5.9192 train_time:121283ms step_avg:1318.30ms
step:103/250 train_loss:5.7568 train_time:122619ms step_avg:1318.48ms
step:104/250 train_loss:5.9112 train_time:123915ms step_avg:1318.25ms
step:105/250 train_loss:5.9272 train_time:125238ms step_avg:1318.29ms
step:106/250 train_loss:6.0886 train_time:126537ms step_avg:1318.09ms
step:107/250 train_loss:5.8682 train_time:127837ms step_avg:1317.90ms
step:108/250 train_loss:5.7600 train_time:129142ms step_avg:1317.77ms
step:109/250 train_loss:6.0228 train_time:130444ms step_avg:1317.62ms
step:110/250 train_loss:5.9586 train_time:131765ms step_avg:1317.65ms
step:111/250 train_loss:5.8322 train_time:133106ms step_avg:1317.88ms
step:112/250 train_loss:5.8915 train_time:134425ms step_avg:1317.89ms
step:113/250 train_loss:5.6677 train_time:135730ms step_avg:1317.77ms
step:114/250 train_loss:5.8916 train_time:137031ms step_avg:1317.61ms
step:115/250 train_loss:5.7668 train_time:138341ms step_avg:1317.53ms
step:116/250 train_loss:5.9377 train_time:139640ms step_avg:1317.36ms
step:117/250 train_loss:5.7445 train_time:140968ms step_avg:1317.46ms
step:118/250 train_loss:5.8024 train_time:142274ms step_avg:1317.35ms
step:119/250 train_loss:5.8154 train_time:143584ms step_avg:1317.28ms
step:120/250 train_loss:5.8715 train_time:144882ms step_avg:1317.11ms
step:121/250 train_loss:5.8193 train_time:146192ms step_avg:1317.04ms
step:122/250 train_loss:5.7228 train_time:147528ms step_avg:1317.21ms
step:123/250 train_loss:5.8380 train_time:148838ms step_avg:1317.15ms
step:124/250 train_loss:5.7098 train_time:150134ms step_avg:1316.97ms
step:125/250 train_loss:5.5632 train_time:151434ms step_avg:1316.82ms
step:125/250 val_loss:5.7427 train_time:151454ms step_avg:1316.99ms
step:126/250 train_loss:5.6001 train_time:152748ms step_avg:1316.79ms
step:127/250 train_loss:5.7681 train_time:154071ms step_avg:1316.85ms
step:128/250 train_loss:5.7954 train_time:155393ms step_avg:1316.89ms
step:129/250 train_loss:5.7599 train_time:156714ms step_avg:1316.92ms
step:130/250 train_loss:5.8197 train_time:158023ms step_avg:1316.86ms
step:131/250 train_loss:5.8814 train_time:159329ms step_avg:1316.77ms
step:132/250 train_loss:5.6280 train_time:160686ms step_avg:1317.10ms
step:133/250 train_loss:5.6184 train_time:162006ms step_avg:1317.13ms
step:134/250 train_loss:5.6806 train_time:163340ms step_avg:1317.26ms
step:135/250 train_loss:5.6091 train_time:164655ms step_avg:1317.24ms
step:136/250 train_loss:5.5662 train_time:165977ms step_avg:1317.28ms
step:137/250 train_loss:5.6675 train_time:167291ms step_avg:1317.25ms
step:138/250 train_loss:5.6354 train_time:168593ms step_avg:1317.13ms
step:139/250 train_loss:5.6599 train_time:169928ms step_avg:1317.28ms
step:140/250 train_loss:5.5425 train_time:171244ms step_avg:1317.26ms
step:141/250 train_loss:5.5621 train_time:172561ms step_avg:1317.26ms
step:142/250 train_loss:5.5525 train_time:173867ms step_avg:1317.17ms
step:143/250 train_loss:5.7595 train_time:175179ms step_avg:1317.14ms
step:144/250 train_loss:6.0940 train_time:176505ms step_avg:1317.20ms
step:145/250 train_loss:5.5928 train_time:177825ms step_avg:1317.22ms
step:146/250 train_loss:5.6595 train_time:179134ms step_avg:1317.16ms
step:147/250 train_loss:5.6392 train_time:180488ms step_avg:1317.43ms
step:148/250 train_loss:5.3617 train_time:181814ms step_avg:1317.49ms
step:149/250 train_loss:5.7936 train_time:183146ms step_avg:1317.60ms
step:150/250 train_loss:5.7101 train_time:184462ms step_avg:1317.59ms
step:151/250 train_loss:5.5299 train_time:185784ms step_avg:1317.61ms
step:152/250 train_loss:5.6534 train_time:187095ms step_avg:1317.57ms
step:153/250 train_loss:5.5466 train_time:188394ms step_avg:1317.44ms
step:154/250 train_loss:5.4813 train_time:189693ms step_avg:1317.31ms
step:155/250 train_loss:5.5079 train_time:191015ms step_avg:1317.35ms
step:156/250 train_loss:5.6047 train_time:192328ms step_avg:1317.32ms
step:157/250 train_loss:5.5740 train_time:193630ms step_avg:1317.21ms
step:158/250 train_loss:5.6036 train_time:194944ms step_avg:1317.19ms
step:159/250 train_loss:5.4700 train_time:196267ms step_avg:1317.23ms
step:160/250 train_loss:5.4638 train_time:197635ms step_avg:1317.57ms
step:161/250 train_loss:5.5191 train_time:198947ms step_avg:1317.53ms
step:162/250 train_loss:5.5440 train_time:200247ms step_avg:1317.41ms
step:163/250 train_loss:5.5083 train_time:201585ms step_avg:1317.55ms
step:164/250 train_loss:5.4068 train_time:202891ms step_avg:1317.48ms
step:165/250 train_loss:5.5064 train_time:204196ms step_avg:1317.39ms
step:166/250 train_loss:5.5974 train_time:205550ms step_avg:1317.63ms
step:167/250 train_loss:5.4486 train_time:206886ms step_avg:1317.74ms
step:168/250 train_loss:5.4731 train_time:208236ms step_avg:1317.95ms
step:169/250 train_loss:5.5073 train_time:209580ms step_avg:1318.11ms
step:170/250 train_loss:5.4828 train_time:210910ms step_avg:1318.19ms
step:171/250 train_loss:4.8352 train_time:212220ms step_avg:1318.13ms
step:172/250 train_loss:5.4031 train_time:213521ms step_avg:1318.03ms
step:173/250 train_loss:5.3742 train_time:214834ms step_avg:1318.00ms
step:174/250 train_loss:5.6065 train_time:216160ms step_avg:1318.05ms
step:175/250 train_loss:5.5753 train_time:217481ms step_avg:1318.07ms
step:176/250 train_loss:5.4790 train_time:218818ms step_avg:1318.18ms
step:177/250 train_loss:5.6471 train_time:220127ms step_avg:1318.12ms
step:178/250 train_loss:5.4160 train_time:221445ms step_avg:1318.12ms
step:179/250 train_loss:5.4007 train_time:222777ms step_avg:1318.21ms
step:180/250 train_loss:5.5011 train_time:224096ms step_avg:1318.21ms
step:181/250 train_loss:5.4362 train_time:225419ms step_avg:1318.24ms
step:182/250 train_loss:5.3079 train_time:226729ms step_avg:1318.19ms
step:183/250 train_loss:5.4949 train_time:228046ms step_avg:1318.18ms
step:184/250 train_loss:5.6339 train_time:229350ms step_avg:1318.10ms
step:185/250 train_loss:5.3796 train_time:230679ms step_avg:1318.16ms
step:186/250 train_loss:5.4579 train_time:232035ms step_avg:1318.38ms
step:187/250 train_loss:5.4664 train_time:233346ms step_avg:1318.34ms
step:188/250 train_loss:5.5040 train_time:234652ms step_avg:1318.27ms
step:189/250 train_loss:5.0081 train_time:235963ms step_avg:1318.23ms
step:190/250 train_loss:5.3301 train_time:237276ms step_avg:1318.20ms
step:191/250 train_loss:5.3409 train_time:238589ms step_avg:1318.17ms
step:192/250 train_loss:5.2953 train_time:239930ms step_avg:1318.30ms
step:193/250 train_loss:5.4441 train_time:241240ms step_avg:1318.25ms
step:194/250 train_loss:5.4111 train_time:242564ms step_avg:1318.28ms
step:195/250 train_loss:5.6238 train_time:243908ms step_avg:1318.42ms
step:196/250 train_loss:5.4955 train_time:245243ms step_avg:1318.51ms
step:197/250 train_loss:5.3383 train_time:246577ms step_avg:1318.59ms
step:198/250 train_loss:5.3545 train_time:247918ms step_avg:1318.71ms
step:199/250 train_loss:5.2913 train_time:249257ms step_avg:1318.82ms
step:200/250 train_loss:5.3795 train_time:250595ms step_avg:1318.92ms
step:201/250 train_loss:5.3176 train_time:251909ms step_avg:1318.90ms
step:202/250 train_loss:5.4812 train_time:253240ms step_avg:1318.96ms
step:203/250 train_loss:5.4385 train_time:254569ms step_avg:1319.01ms
step:204/250 train_loss:5.3478 train_time:255909ms step_avg:1319.12ms
step:205/250 train_loss:5.5371 train_time:257247ms step_avg:1319.21ms
step:206/250 train_loss:5.2225 train_time:258583ms step_avg:1319.30ms
step:207/250 train_loss:5.3713 train_time:259891ms step_avg:1319.24ms
step:208/250 train_loss:5.3321 train_time:261235ms step_avg:1319.37ms
step:209/250 train_loss:5.4902 train_time:262569ms step_avg:1319.44ms
step:210/250 train_loss:5.3539 train_time:263882ms step_avg:1319.41ms
step:211/250 train_loss:5.3000 train_time:265204ms step_avg:1319.42ms
step:212/250 train_loss:5.5392 train_time:266521ms step_avg:1319.41ms
step:213/250 train_loss:5.2734 train_time:267826ms step_avg:1319.34ms
step:214/250 train_loss:5.3690 train_time:269161ms step_avg:1319.42ms
step:215/250 train_loss:5.2623 train_time:270478ms step_avg:1319.40ms
step:216/250 train_loss:5.3591 train_time:271810ms step_avg:1319.47ms
step:217/250 train_loss:5.3474 train_time:273128ms step_avg:1319.46ms
step:218/250 train_loss:5.2874 train_time:274449ms step_avg:1319.47ms
step:219/250 train_loss:5.3255 train_time:275781ms step_avg:1319.53ms
step:220/250 train_loss:5.3279 train_time:277097ms step_avg:1319.51ms
step:221/250 train_loss:5.4089 train_time:278426ms step_avg:1319.56ms
step:222/250 train_loss:5.3917 train_time:279740ms step_avg:1319.53ms
step:223/250 train_loss:5.3786 train_time:281070ms step_avg:1319.58ms
step:224/250 train_loss:5.4269 train_time:282401ms step_avg:1319.63ms
step:225/250 train_loss:5.1371 train_time:283715ms step_avg:1319.60ms
step:226/250 train_loss:5.2563 train_time:285017ms step_avg:1319.52ms
step:227/250 train_loss:5.2204 train_time:286322ms step_avg:1319.46ms
step:228/250 train_loss:5.3811 train_time:287657ms step_avg:1319.53ms
step:229/250 train_loss:5.2773 train_time:288971ms step_avg:1319.50ms
step:230/250 train_loss:5.4212 train_time:290278ms step_avg:1319.44ms
step:231/250 train_loss:5.3085 train_time:291604ms step_avg:1319.48ms
step:232/250 train_loss:5.1836 train_time:292934ms step_avg:1319.52ms
step:233/250 train_loss:5.4273 train_time:294237ms step_avg:1319.45ms
step:234/250 train_loss:5.2674 train_time:295542ms step_avg:1319.38ms
step:235/250 train_loss:5.1602 train_time:296865ms step_avg:1319.40ms
step:236/250 train_loss:5.4899 train_time:298180ms step_avg:1319.38ms
step:237/250 train_loss:5.3075 train_time:299484ms step_avg:1319.31ms
step:238/250 train_loss:5.2707 train_time:300815ms step_avg:1319.36ms
step:239/250 train_loss:5.4116 train_time:302115ms step_avg:1319.28ms
step:240/250 train_loss:5.4098 train_time:303425ms step_avg:1319.24ms
step:241/250 train_loss:5.3076 train_time:304753ms step_avg:1319.28ms
step:242/250 train_loss:5.4655 train_time:306080ms step_avg:1319.31ms
step:243/250 train_loss:5.2761 train_time:307392ms step_avg:1319.28ms
step:244/250 train_loss:5.2532 train_time:308727ms step_avg:1319.34ms
step:245/250 train_loss:5.3488 train_time:310038ms step_avg:1319.31ms
step:246/250 train_loss:5.3154 train_time:311366ms step_avg:1319.35ms
step:247/250 train_loss:5.3137 train_time:312700ms step_avg:1319.41ms
step:248/250 train_loss:5.5129 train_time:314009ms step_avg:1319.36ms
step:249/250 train_loss:5.2148 train_time:315361ms step_avg:1319.50ms
step:250/250 train_loss:5.2422 train_time:316700ms step_avg:1319.58ms
step:250/250 val_loss:5.3181 train_time:316754ms step_avg:1319.81ms
