====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config, layer_idx: int):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        
        # flags from config
        self.use_mdha = getattr(config, "use_mdha", False)
        self.use_gqa  = getattr(config, "use_gqa", False)
        self.num_gqa_layers = getattr(config, "num_gqa_layers", 0)

        self.is_gqa_layer = (
            self.use_gqa and
            self.num_gqa_layers > 0 and
            layer_idx >= config.n_layer - self.num_gqa_layers
        )

        # K/V groups: in lower layers, num_kv_groups == n_head (baseline),
        # in upper GQA layers, num_kv_groups < n_head (e.g., 4 groups for 16 heads).
        self.num_kv_groups = (
            getattr(config, "num_kv_groups", self.n_head)
            if self.is_gqa_layer else
            self.n_head
        )

        # Q keeps full heads always
        self.c_q = nn.Linear(self.n_embd, self.n_head * self.head_dim, bias=False)
        # K/V possibly reduced to num_kv_groups
        self.c_k = nn.Linear(self.n_embd, self.num_kv_groups * self.head_dim, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.num_kv_groups * self.head_dim, bias=False)

        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_()  # zero init suggested by @Grad62304977

        self.rotary = Rotary(self.head_dim)

        # MDHA convs: depthwise-ish Conv1d along time
        if self.use_mdha:
            ksize = getattr(config, "mdha_kernel_size", 3)

            # For Q: conv over [B, H*D, T], grouped by head
            self.q_conv = nn.Conv1d(
                in_channels=self.n_head * self.head_dim,
                out_channels=self.n_head * self.head_dim,
                kernel_size=ksize,
                groups=self.n_head,
                padding=ksize - 1,
            )

            # For K/V: conv over [B, G*D, T], grouped by KV group
            kv_channels = self.num_kv_groups * self.head_dim
            self.kv_conv = nn.Conv1d(
                in_channels=kv_channels,
                out_channels=kv_channels,
                kernel_size=ksize,
                groups=self.num_kv_groups,
                padding=ksize - 1,
            )


    def forward(self, x):
        B, T, C = x.size()

        # 1) Project Q/K/V with possibly different numbers of K/V groups
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.num_kv_groups, self.head_dim)
        v = self.c_v(x).view(B, T, self.num_kv_groups, self.head_dim)

        # 2) Optional MDHA (Conv1d along time) on Q/K/V
        if self.use_mdha:
            # Q: [B, T, H, D] -> [B, H*D, T] -> conv -> back
            q_flat = q.reshape(B, T, self.n_head * self.head_dim).transpose(1, 2)
            q_flat = self.q_conv(q_flat)[..., :T]  # crop to keep sequence length T
            q = q_flat.transpose(1, 2).reshape(B, T, self.n_head, self.head_dim)

            # K/V: [B, T, G, D] -> [B, G*D, T] -> conv -> back
            kv_channels = self.num_kv_groups * self.head_dim
            k_flat = k.reshape(B, T, kv_channels).transpose(1, 2)
            v_flat = v.reshape(B, T, kv_channels).transpose(1, 2)
            k_flat = self.kv_conv(k_flat)[..., :T]
            v_flat = self.kv_conv(v_flat)[..., :T]
            k = k_flat.transpose(1, 2).reshape(B, T, self.num_kv_groups, self.head_dim)
            v = v_flat.transpose(1, 2).reshape(B, T, self.num_kv_groups, self.head_dim)

        # 3) Rotary embeddings + QK norm (same as baseline, but K has G groups instead of H)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),))

        # 4) Prepare for scaled dot-product attention
        # q_attn: [B, H, T, D]
        q_attn = q.transpose(1, 2)

        # k/v: either baseline (num_kv_groups == n_head) or GQA (num_kv_groups < n_head)
        if self.is_gqa_layer:
            assert self.n_head % self.num_kv_groups == 0, \
        f"n_head ({self.n_head}) must be divisible by num_kv_groups ({self.num_kv_groups}) in GQA layers"

            # GQA: repeat each KV group across its group of heads
            group_size = self.n_head // self.num_kv_groups
            k_group = k.transpose(1, 2)  # [B, G, T, D]
            v_group = v.transpose(1, 2)  # [B, G, T, D]
            k_attn = k_group.repeat_interleave(group_size, dim=1)
            v_attn = v_group.repeat_interleave(group_size, dim=1)
        else:
            # baseline: num_kv_groups == n_head
            k_attn = k.transpose(1, 2)
            v_attn = v.transpose(1, 2)

        # 5) Scaled dot-product attention (unchanged interface)
        y = F.scaled_dot_product_attention(
            q_attn, k_attn, v_attn,
            is_causal=True,
        )
        # 6) Re-assemble heads and project out
        y = y.transpose(1, 2).contiguous().view_as(x)
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config, layer_idx):
        super().__init__()
        self.layer_idx = layer_idx
        self.attn = CausalSelfAttention(config, layer_idx=layer_idx)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    
    # attention experiment flags
    use_mdha : bool = False
    use_gqa : bool = False
    mdha_kernel_size : int = 3
    num_kv_groups : int = 1
    num_gqa_layers : int = 0


class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config, layer_idx=i) for i in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float()
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :])
            logits = logits.float()
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 250 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end

    # attention experiment flags (wired to env vars for CLI control)
    use_mdha : bool = bool(int(os.getenv("USE_MDHA", "0")))
    use_gqa : bool = bool(int(os.getenv("USE_GQA", "0")))
    mdha_kernel_size : int = int(os.getenv("MDHA_KERNEL_SIZE", "3"))
    num_kv_groups : int = int(os.getenv("NUM_KV_GROUPS", "1"))
    num_gqa_layers : int = int(os.getenv("NUM_GQA_LAYERS", "0"))

args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(
    vocab_size=num_vocab,
    n_layer=12,
    n_head=6,
    n_embd=768,
    use_mdha=args.use_mdha,
    use_gqa=args.use_gqa,
    mdha_kernel_size=args.mdha_kernel_size,
    num_kv_groups=args.num_kv_groups,
    num_gqa_layers=args.num_gqa_layers,
))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
if master_process:
    print(f"use_mdha={args.use_mdha}, use_gqa={args.use_gqa}, "
          f"mdha_kernel_size={args.mdha_kernel_size}, "
          f"num_kv_groups={args.num_kv_groups}, num_gqa_layers={args.num_gqa_layers}")
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
# optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
#                                weight_decay=args.weight_decay, fused=True)
# optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
# optimizers = [optimizer1, optimizer2]
# collect transformer.h params into Muon-safe (2D) and the rest
muon_params = []
extra_adam_params = []

for name, p in raw_model.transformer.h.named_parameters():
    if p.ndim == 2:
        muon_params.append(p)
    else:
        extra_adam_params.append(p)

# AdamW: lm_head + any non-2D transformer params (e.g., conv weights, biases)
optimizer1 = torch.optim.AdamW(
    list(raw_model.lm_head.parameters()) + extra_adam_params,
    lr=args.learning_rate,
    betas=(0.9, 0.95),
    weight_decay=args.weight_decay,
    fused=True,
)

# Muon: only 2D parameters in the transformer blocks
optimizer2 = Muon(muon_params, lr=0.1*args.learning_rate, momentum=0.95)

optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

        

        f.write("Experiment config:\n")
        f.write(f"  use_mdha       = {args.use_mdha}\n")
        f.write(f"  mdha_kernel_sz = {args.mdha_kernel_size}\n")
        f.write(f"  use_gqa        = {args.use_gqa}\n")
        f.write(f"  num_kv_groups  = {args.num_kv_groups}\n")
        f.write(f"  num_gqa_layers = {args.num_gqa_layers}\n")
        f.write('='*100 + '\n')


training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            _, loss = model(x, y, return_logits=False)
            train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Mon Dec  8 17:46:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:90:00.0 Off |                    0 |
| N/A   45C    P0             91W /  400W |    2517MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:B7:00.0 Off |                    0 |
| N/A   32C    P0             84W /  400W |    2517MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Experiment config:
  use_mdha       = False
  mdha_kernel_sz = 3
  use_gqa        = True
  num_kv_groups  = 1
  num_gqa_layers = 4
====================================================================================================
step:0/250 val_loss:16.0337 train_time:247ms step_avg:nanms
step:1/250 train_loss:16.0369 train_time:115178ms step_avg:nanms
step:2/250 train_loss:10.6693 train_time:116444ms step_avg:nanms
step:3/250 train_loss:9.4632 train_time:117724ms step_avg:nanms
step:4/250 train_loss:8.8806 train_time:119036ms step_avg:nanms
step:5/250 train_loss:8.6638 train_time:120310ms step_avg:nanms
step:6/250 train_loss:8.3539 train_time:121591ms step_avg:nanms
step:7/250 train_loss:8.1775 train_time:122877ms step_avg:nanms
step:8/250 train_loss:7.7200 train_time:124171ms step_avg:nanms
step:9/250 train_loss:7.7919 train_time:125449ms step_avg:nanms
step:10/250 train_loss:7.5571 train_time:126722ms step_avg:nanms
step:11/250 train_loss:7.4327 train_time:1264ms step_avg:nanms
step:12/250 train_loss:7.3045 train_time:2537ms step_avg:nanms
step:13/250 train_loss:7.2496 train_time:3843ms step_avg:1281.14ms
step:14/250 train_loss:7.1620 train_time:5129ms step_avg:1282.36ms
step:15/250 train_loss:7.2105 train_time:6407ms step_avg:1281.40ms
step:16/250 train_loss:7.0218 train_time:7694ms step_avg:1282.33ms
step:17/250 train_loss:6.9841 train_time:8985ms step_avg:1283.62ms
step:18/250 train_loss:7.4167 train_time:10271ms step_avg:1283.81ms
step:19/250 train_loss:6.9829 train_time:11567ms step_avg:1285.18ms
step:20/250 train_loss:7.1335 train_time:12854ms step_avg:1285.44ms
step:21/250 train_loss:6.9137 train_time:14143ms step_avg:1285.75ms
step:22/250 train_loss:6.7949 train_time:15435ms step_avg:1286.21ms
step:23/250 train_loss:6.8194 train_time:16729ms step_avg:1286.88ms
step:24/250 train_loss:6.8054 train_time:18044ms step_avg:1288.88ms
step:25/250 train_loss:6.5981 train_time:19330ms step_avg:1288.68ms
step:26/250 train_loss:6.7928 train_time:20637ms step_avg:1289.82ms
step:27/250 train_loss:6.7098 train_time:21922ms step_avg:1289.54ms
step:28/250 train_loss:6.7193 train_time:23213ms step_avg:1289.63ms
step:29/250 train_loss:6.6635 train_time:24523ms step_avg:1290.71ms
step:30/250 train_loss:6.7350 train_time:25823ms step_avg:1291.17ms
step:31/250 train_loss:7.1263 train_time:27117ms step_avg:1291.30ms
step:32/250 train_loss:6.5624 train_time:28408ms step_avg:1291.27ms
step:33/250 train_loss:6.4641 train_time:29747ms step_avg:1293.34ms
step:34/250 train_loss:6.5066 train_time:31093ms step_avg:1295.55ms
step:35/250 train_loss:6.6694 train_time:32381ms step_avg:1295.26ms
step:36/250 train_loss:6.6630 train_time:33679ms step_avg:1295.34ms
step:37/250 train_loss:6.6067 train_time:34994ms step_avg:1296.09ms
step:38/250 train_loss:6.4759 train_time:36293ms step_avg:1296.18ms
step:39/250 train_loss:6.5683 train_time:37610ms step_avg:1296.89ms
step:40/250 train_loss:6.3821 train_time:38903ms step_avg:1296.78ms
step:41/250 train_loss:6.5354 train_time:40213ms step_avg:1297.19ms
step:42/250 train_loss:6.4465 train_time:41519ms step_avg:1297.47ms
step:43/250 train_loss:6.4326 train_time:42825ms step_avg:1297.72ms
step:44/250 train_loss:6.3845 train_time:44113ms step_avg:1297.44ms
step:45/250 train_loss:6.2832 train_time:45419ms step_avg:1297.68ms
step:46/250 train_loss:6.3495 train_time:46729ms step_avg:1298.02ms
step:47/250 train_loss:6.2775 train_time:48073ms step_avg:1299.27ms
step:48/250 train_loss:6.4483 train_time:49377ms step_avg:1299.39ms
step:49/250 train_loss:6.1954 train_time:50678ms step_avg:1299.42ms
step:50/250 train_loss:6.3529 train_time:51974ms step_avg:1299.34ms
step:51/250 train_loss:6.3941 train_time:53280ms step_avg:1299.52ms
step:52/250 train_loss:6.4015 train_time:54581ms step_avg:1299.54ms
step:53/250 train_loss:6.3258 train_time:55879ms step_avg:1299.50ms
step:54/250 train_loss:6.3441 train_time:57203ms step_avg:1300.08ms
step:55/250 train_loss:6.2354 train_time:58494ms step_avg:1299.87ms
step:56/250 train_loss:6.2126 train_time:59784ms step_avg:1299.65ms
step:57/250 train_loss:6.2513 train_time:61084ms step_avg:1299.66ms
step:58/250 train_loss:6.2645 train_time:62370ms step_avg:1299.37ms
step:59/250 train_loss:6.3361 train_time:63660ms step_avg:1299.18ms
step:60/250 train_loss:6.1838 train_time:64953ms step_avg:1299.07ms
step:61/250 train_loss:6.2668 train_time:66249ms step_avg:1298.99ms
step:62/250 train_loss:6.3001 train_time:67540ms step_avg:1298.84ms
step:63/250 train_loss:6.2222 train_time:68834ms step_avg:1298.76ms
step:64/250 train_loss:6.1909 train_time:70125ms step_avg:1298.61ms
step:65/250 train_loss:6.0158 train_time:71426ms step_avg:1298.65ms
step:66/250 train_loss:6.0325 train_time:72715ms step_avg:1298.49ms
step:67/250 train_loss:6.2485 train_time:74002ms step_avg:1298.27ms
step:68/250 train_loss:6.2148 train_time:75284ms step_avg:1298.00ms
step:69/250 train_loss:6.2598 train_time:76577ms step_avg:1297.91ms
step:70/250 train_loss:6.0841 train_time:77866ms step_avg:1297.77ms
step:71/250 train_loss:6.2665 train_time:79152ms step_avg:1297.58ms
step:72/250 train_loss:6.1799 train_time:80445ms step_avg:1297.51ms
step:73/250 train_loss:6.2196 train_time:81743ms step_avg:1297.50ms
step:74/250 train_loss:5.9383 train_time:83029ms step_avg:1297.33ms
step:75/250 train_loss:6.0332 train_time:84335ms step_avg:1297.46ms
step:76/250 train_loss:6.0106 train_time:85676ms step_avg:1298.12ms
step:77/250 train_loss:6.1830 train_time:86979ms step_avg:1298.19ms
step:78/250 train_loss:6.0648 train_time:88278ms step_avg:1298.21ms
step:79/250 train_loss:5.6410 train_time:89575ms step_avg:1298.18ms
step:80/250 train_loss:6.0813 train_time:90868ms step_avg:1298.11ms
step:81/250 train_loss:6.1159 train_time:92150ms step_avg:1297.89ms
step:82/250 train_loss:6.0360 train_time:93482ms step_avg:1298.37ms
step:83/250 train_loss:6.1286 train_time:94772ms step_avg:1298.25ms
step:84/250 train_loss:6.0009 train_time:96094ms step_avg:1298.57ms
step:85/250 train_loss:6.0835 train_time:97388ms step_avg:1298.50ms
step:86/250 train_loss:6.0272 train_time:98706ms step_avg:1298.77ms
step:87/250 train_loss:6.1111 train_time:99992ms step_avg:1298.60ms
step:88/250 train_loss:5.9435 train_time:101337ms step_avg:1299.19ms
step:89/250 train_loss:5.9313 train_time:102620ms step_avg:1298.98ms
step:90/250 train_loss:5.7745 train_time:103929ms step_avg:1299.12ms
step:91/250 train_loss:6.0319 train_time:105248ms step_avg:1299.36ms
step:92/250 train_loss:5.9782 train_time:106538ms step_avg:1299.24ms
step:93/250 train_loss:6.2326 train_time:107827ms step_avg:1299.13ms
step:94/250 train_loss:6.0999 train_time:109139ms step_avg:1299.28ms
step:95/250 train_loss:5.8558 train_time:110428ms step_avg:1299.16ms
step:96/250 train_loss:5.9411 train_time:111740ms step_avg:1299.30ms
step:97/250 train_loss:6.0562 train_time:113043ms step_avg:1299.34ms
step:98/250 train_loss:5.8452 train_time:114349ms step_avg:1299.43ms
step:99/250 train_loss:5.8333 train_time:115702ms step_avg:1300.02ms
step:100/250 train_loss:5.8498 train_time:116992ms step_avg:1299.92ms
step:101/250 train_loss:5.7191 train_time:118293ms step_avg:1299.92ms
step:102/250 train_loss:5.9057 train_time:119596ms step_avg:1299.96ms
step:103/250 train_loss:5.7433 train_time:120889ms step_avg:1299.88ms
step:104/250 train_loss:5.9048 train_time:122215ms step_avg:1300.16ms
step:105/250 train_loss:5.9124 train_time:123504ms step_avg:1300.04ms
step:106/250 train_loss:6.0782 train_time:124810ms step_avg:1300.10ms
step:107/250 train_loss:5.8623 train_time:126102ms step_avg:1300.02ms
step:108/250 train_loss:5.7476 train_time:127397ms step_avg:1299.97ms
step:109/250 train_loss:6.0075 train_time:128686ms step_avg:1299.86ms
step:110/250 train_loss:5.9559 train_time:129984ms step_avg:1299.84ms
step:111/250 train_loss:5.8243 train_time:131278ms step_avg:1299.78ms
step:112/250 train_loss:5.8947 train_time:132569ms step_avg:1299.70ms
step:113/250 train_loss:5.6555 train_time:133860ms step_avg:1299.61ms
step:114/250 train_loss:5.8837 train_time:135159ms step_avg:1299.61ms
step:115/250 train_loss:5.7571 train_time:136450ms step_avg:1299.53ms
step:116/250 train_loss:5.9249 train_time:137753ms step_avg:1299.55ms
step:117/250 train_loss:5.7312 train_time:139060ms step_avg:1299.63ms
step:118/250 train_loss:5.7944 train_time:140346ms step_avg:1299.50ms
step:119/250 train_loss:5.8079 train_time:141662ms step_avg:1299.65ms
step:120/250 train_loss:5.8579 train_time:142965ms step_avg:1299.68ms
step:121/250 train_loss:5.8114 train_time:144284ms step_avg:1299.86ms
step:122/250 train_loss:5.7158 train_time:145580ms step_avg:1299.82ms
step:123/250 train_loss:5.8327 train_time:146871ms step_avg:1299.75ms
step:124/250 train_loss:5.7020 train_time:148167ms step_avg:1299.71ms
step:125/250 train_loss:5.5591 train_time:149461ms step_avg:1299.66ms
step:125/250 val_loss:5.7358 train_time:149487ms step_avg:1299.88ms
step:126/250 train_loss:5.5969 train_time:150766ms step_avg:1299.71ms
step:127/250 train_loss:5.7603 train_time:152062ms step_avg:1299.67ms
step:128/250 train_loss:5.7888 train_time:153360ms step_avg:1299.66ms
step:129/250 train_loss:5.7533 train_time:154645ms step_avg:1299.54ms
step:130/250 train_loss:5.8139 train_time:155941ms step_avg:1299.51ms
step:131/250 train_loss:5.8740 train_time:157247ms step_avg:1299.56ms
step:132/250 train_loss:5.6204 train_time:158528ms step_avg:1299.41ms
step:133/250 train_loss:5.6043 train_time:159812ms step_avg:1299.28ms
step:134/250 train_loss:5.6681 train_time:161098ms step_avg:1299.18ms
step:135/250 train_loss:5.6037 train_time:162384ms step_avg:1299.07ms
step:136/250 train_loss:5.5591 train_time:163677ms step_avg:1299.02ms
step:137/250 train_loss:5.6599 train_time:164983ms step_avg:1299.08ms
step:138/250 train_loss:5.6296 train_time:166281ms step_avg:1299.07ms
step:139/250 train_loss:5.6540 train_time:167587ms step_avg:1299.12ms
step:140/250 train_loss:5.5356 train_time:168889ms step_avg:1299.15ms
step:141/250 train_loss:5.5511 train_time:170186ms step_avg:1299.13ms
step:142/250 train_loss:5.5509 train_time:171479ms step_avg:1299.08ms
step:143/250 train_loss:5.7531 train_time:172773ms step_avg:1299.04ms
step:144/250 train_loss:6.0920 train_time:174082ms step_avg:1299.12ms
step:145/250 train_loss:5.5902 train_time:175381ms step_avg:1299.12ms
step:146/250 train_loss:5.6488 train_time:176679ms step_avg:1299.11ms
step:147/250 train_loss:5.6330 train_time:177964ms step_avg:1299.01ms
step:148/250 train_loss:5.3554 train_time:179261ms step_avg:1298.99ms
step:149/250 train_loss:5.7915 train_time:180563ms step_avg:1299.02ms
step:150/250 train_loss:5.7112 train_time:181877ms step_avg:1299.12ms
step:151/250 train_loss:5.5270 train_time:183181ms step_avg:1299.15ms
step:152/250 train_loss:5.6584 train_time:184469ms step_avg:1299.08ms
step:153/250 train_loss:5.5437 train_time:185776ms step_avg:1299.14ms
step:154/250 train_loss:5.4753 train_time:187081ms step_avg:1299.17ms
step:155/250 train_loss:5.5009 train_time:188424ms step_avg:1299.48ms
step:156/250 train_loss:5.5995 train_time:189733ms step_avg:1299.54ms
step:157/250 train_loss:5.5686 train_time:191036ms step_avg:1299.56ms
step:158/250 train_loss:5.5907 train_time:192349ms step_avg:1299.65ms
step:159/250 train_loss:5.4629 train_time:193672ms step_avg:1299.82ms
step:160/250 train_loss:5.4496 train_time:194990ms step_avg:1299.94ms
step:161/250 train_loss:5.5066 train_time:196295ms step_avg:1299.97ms
step:162/250 train_loss:5.5414 train_time:197582ms step_avg:1299.88ms
step:163/250 train_loss:5.4943 train_time:198871ms step_avg:1299.81ms
step:164/250 train_loss:5.4017 train_time:200193ms step_avg:1299.95ms
step:165/250 train_loss:5.4930 train_time:201505ms step_avg:1300.04ms
step:166/250 train_loss:5.5855 train_time:202807ms step_avg:1300.04ms
step:167/250 train_loss:5.4377 train_time:204102ms step_avg:1300.02ms
step:168/250 train_loss:5.4602 train_time:205390ms step_avg:1299.94ms
step:169/250 train_loss:5.5010 train_time:206682ms step_avg:1299.89ms
step:170/250 train_loss:5.4751 train_time:207974ms step_avg:1299.84ms
step:171/250 train_loss:4.8339 train_time:209295ms step_avg:1299.97ms
step:172/250 train_loss:5.3959 train_time:210609ms step_avg:1300.05ms
step:173/250 train_loss:5.3690 train_time:211900ms step_avg:1300.00ms
step:174/250 train_loss:5.5993 train_time:213196ms step_avg:1299.98ms
step:175/250 train_loss:5.5694 train_time:214494ms step_avg:1299.96ms
step:176/250 train_loss:5.4730 train_time:215809ms step_avg:1300.05ms
step:177/250 train_loss:5.6407 train_time:217099ms step_avg:1300.00ms
step:178/250 train_loss:5.4179 train_time:218391ms step_avg:1299.95ms
step:179/250 train_loss:5.3911 train_time:219676ms step_avg:1299.86ms
step:180/250 train_loss:5.4956 train_time:220987ms step_avg:1299.92ms
step:181/250 train_loss:5.4347 train_time:222279ms step_avg:1299.88ms
step:182/250 train_loss:5.3022 train_time:223568ms step_avg:1299.82ms
step:183/250 train_loss:5.4905 train_time:224885ms step_avg:1299.91ms
step:184/250 train_loss:5.6271 train_time:226185ms step_avg:1299.92ms
step:185/250 train_loss:5.3730 train_time:227474ms step_avg:1299.85ms
step:186/250 train_loss:5.4561 train_time:228783ms step_avg:1299.91ms
step:187/250 train_loss:5.4616 train_time:230075ms step_avg:1299.86ms
step:188/250 train_loss:5.4965 train_time:231397ms step_avg:1299.98ms
step:189/250 train_loss:5.0042 train_time:232690ms step_avg:1299.95ms
step:190/250 train_loss:5.3245 train_time:233978ms step_avg:1299.88ms
step:191/250 train_loss:5.3338 train_time:235277ms step_avg:1299.88ms
step:192/250 train_loss:5.2891 train_time:236598ms step_avg:1299.99ms
step:193/250 train_loss:5.4327 train_time:237893ms step_avg:1299.96ms
step:194/250 train_loss:5.4048 train_time:239188ms step_avg:1299.93ms
step:195/250 train_loss:5.6207 train_time:240480ms step_avg:1299.89ms
step:196/250 train_loss:5.4935 train_time:241795ms step_avg:1299.97ms
step:197/250 train_loss:5.3313 train_time:243083ms step_avg:1299.91ms
step:198/250 train_loss:5.3470 train_time:244381ms step_avg:1299.90ms
step:199/250 train_loss:5.2828 train_time:245680ms step_avg:1299.90ms
step:200/250 train_loss:5.3732 train_time:246984ms step_avg:1299.91ms
step:201/250 train_loss:5.3132 train_time:248270ms step_avg:1299.85ms
step:202/250 train_loss:5.4805 train_time:249578ms step_avg:1299.88ms
step:203/250 train_loss:5.4298 train_time:250873ms step_avg:1299.86ms
step:204/250 train_loss:5.3387 train_time:252176ms step_avg:1299.88ms
step:205/250 train_loss:5.5336 train_time:253477ms step_avg:1299.88ms
step:206/250 train_loss:5.2139 train_time:254772ms step_avg:1299.86ms
step:207/250 train_loss:5.3631 train_time:256078ms step_avg:1299.89ms
step:208/250 train_loss:5.3205 train_time:257398ms step_avg:1299.99ms
step:209/250 train_loss:5.4813 train_time:258702ms step_avg:1300.01ms
step:210/250 train_loss:5.3499 train_time:259992ms step_avg:1299.96ms
step:211/250 train_loss:5.2923 train_time:261277ms step_avg:1299.89ms
step:212/250 train_loss:5.5221 train_time:262584ms step_avg:1299.92ms
step:213/250 train_loss:5.2706 train_time:263876ms step_avg:1299.88ms
step:214/250 train_loss:5.3651 train_time:265178ms step_avg:1299.89ms
step:215/250 train_loss:5.2546 train_time:266486ms step_avg:1299.93ms
step:216/250 train_loss:5.3619 train_time:267774ms step_avg:1299.87ms
step:217/250 train_loss:5.3373 train_time:269095ms step_avg:1299.98ms
step:218/250 train_loss:5.2816 train_time:270381ms step_avg:1299.91ms
step:219/250 train_loss:5.3160 train_time:271684ms step_avg:1299.92ms
step:220/250 train_loss:5.3203 train_time:272986ms step_avg:1299.93ms
step:221/250 train_loss:5.4051 train_time:274276ms step_avg:1299.89ms
step:222/250 train_loss:5.3928 train_time:275565ms step_avg:1299.83ms
step:223/250 train_loss:5.3773 train_time:276862ms step_avg:1299.82ms
step:224/250 train_loss:5.4185 train_time:278148ms step_avg:1299.76ms
step:225/250 train_loss:5.1324 train_time:279429ms step_avg:1299.67ms
step:226/250 train_loss:5.2477 train_time:280712ms step_avg:1299.59ms
step:227/250 train_loss:5.2096 train_time:282006ms step_avg:1299.57ms
step:228/250 train_loss:5.3734 train_time:283294ms step_avg:1299.52ms
step:229/250 train_loss:5.2669 train_time:284576ms step_avg:1299.44ms
step:230/250 train_loss:5.4162 train_time:285900ms step_avg:1299.54ms
step:231/250 train_loss:5.3008 train_time:287189ms step_avg:1299.50ms
step:232/250 train_loss:5.1782 train_time:288474ms step_avg:1299.43ms
step:233/250 train_loss:5.4216 train_time:289766ms step_avg:1299.40ms
step:234/250 train_loss:5.2618 train_time:291080ms step_avg:1299.46ms
step:235/250 train_loss:5.1635 train_time:292397ms step_avg:1299.54ms
step:236/250 train_loss:5.4827 train_time:293681ms step_avg:1299.47ms
step:237/250 train_loss:5.3024 train_time:294968ms step_avg:1299.42ms
step:238/250 train_loss:5.2634 train_time:296261ms step_avg:1299.39ms
step:239/250 train_loss:5.4042 train_time:297548ms step_avg:1299.34ms
step:240/250 train_loss:5.4054 train_time:298861ms step_avg:1299.40ms
step:241/250 train_loss:5.2977 train_time:300151ms step_avg:1299.36ms
step:242/250 train_loss:5.4634 train_time:301448ms step_avg:1299.35ms
step:243/250 train_loss:5.2679 train_time:302741ms step_avg:1299.32ms
step:244/250 train_loss:5.2505 train_time:304041ms step_avg:1299.32ms
step:245/250 train_loss:5.3395 train_time:305322ms step_avg:1299.24ms
step:246/250 train_loss:5.3093 train_time:306610ms step_avg:1299.20ms
step:247/250 train_loss:5.3073 train_time:307900ms step_avg:1299.15ms
step:248/250 train_loss:5.5035 train_time:309185ms step_avg:1299.10ms
step:249/250 train_loss:5.2075 train_time:310471ms step_avg:1299.04ms
step:250/250 train_loss:5.2365 train_time:311756ms step_avg:1298.98ms
step:250/250 val_loss:5.3119 train_time:311763ms step_avg:1299.01ms
