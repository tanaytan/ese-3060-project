====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config, layer_idx: int):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        
        # flags from config
        self.use_mdha = getattr(config, "use_mdha", False)
        self.use_gqa  = getattr(config, "use_gqa", False)
        self.num_gqa_layers = getattr(config, "num_gqa_layers", 0)

        self.is_gqa_layer = (
            self.use_gqa and
            self.num_gqa_layers > 0 and
            layer_idx >= config.n_layer - self.num_gqa_layers
        )

        # K/V groups: in lower layers, num_kv_groups == n_head (baseline),
        # in upper GQA layers, num_kv_groups < n_head (e.g., 4 groups for 16 heads).
        self.num_kv_groups = (
            getattr(config, "num_kv_groups", self.n_head)
            if self.is_gqa_layer else
            self.n_head
        )

        # Q keeps full heads always
        self.c_q = nn.Linear(self.n_embd, self.n_head * self.head_dim, bias=False)
        # K/V possibly reduced to num_kv_groups
        self.c_k = nn.Linear(self.n_embd, self.num_kv_groups * self.head_dim, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.num_kv_groups * self.head_dim, bias=False)

        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_()  # zero init suggested by @Grad62304977

        self.rotary = Rotary(self.head_dim)

        # MDHA convs: depthwise-ish Conv1d along time
        if self.use_mdha:
            ksize = getattr(config, "mdha_kernel_size", 3)

            # For Q: conv over [B, H*D, T], grouped by head
            self.q_conv = nn.Conv1d(
                in_channels=self.n_head * self.head_dim,
                out_channels=self.n_head * self.head_dim,
                kernel_size=ksize,
                groups=self.n_head,
                padding=ksize - 1,
            )

            # For K/V: conv over [B, G*D, T], grouped by KV group
            kv_channels = self.num_kv_groups * self.head_dim
            self.kv_conv = nn.Conv1d(
                in_channels=kv_channels,
                out_channels=kv_channels,
                kernel_size=ksize,
                groups=self.num_kv_groups,
                padding=ksize - 1,
            )


    def forward(self, x):
        B, T, C = x.size()

        # 1) Project Q/K/V with possibly different numbers of K/V groups
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.num_kv_groups, self.head_dim)
        v = self.c_v(x).view(B, T, self.num_kv_groups, self.head_dim)

        # 2) Optional MDHA (Conv1d along time) on Q/K/V
        if self.use_mdha:
            # Q: [B, T, H, D] -> [B, H*D, T] -> conv -> back
            q_flat = q.reshape(B, T, self.n_head * self.head_dim).transpose(1, 2)
            q_flat = self.q_conv(q_flat)[..., :T]  # crop to keep sequence length T
            q = q_flat.transpose(1, 2).reshape(B, T, self.n_head, self.head_dim)

            # K/V: [B, T, G, D] -> [B, G*D, T] -> conv -> back
            kv_channels = self.num_kv_groups * self.head_dim
            k_flat = k.reshape(B, T, kv_channels).transpose(1, 2)
            v_flat = v.reshape(B, T, kv_channels).transpose(1, 2)
            k_flat = self.kv_conv(k_flat)[..., :T]
            v_flat = self.kv_conv(v_flat)[..., :T]
            k = k_flat.transpose(1, 2).reshape(B, T, self.num_kv_groups, self.head_dim)
            v = v_flat.transpose(1, 2).reshape(B, T, self.num_kv_groups, self.head_dim)

        # 3) Rotary embeddings + QK norm (same as baseline, but K has G groups instead of H)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),))

        # 4) Prepare for scaled dot-product attention
        # q_attn: [B, H, T, D]
        q_attn = q.transpose(1, 2)

        # k/v: either baseline (num_kv_groups == n_head) or GQA (num_kv_groups < n_head)
        if self.is_gqa_layer:
            assert self.n_head % self.num_kv_groups == 0, \
        f"n_head ({self.n_head}) must be divisible by num_kv_groups ({self.num_kv_groups}) in GQA layers"

            # GQA: repeat each KV group across its group of heads
            group_size = self.n_head // self.num_kv_groups
            k_group = k.transpose(1, 2)  # [B, G, T, D]
            v_group = v.transpose(1, 2)  # [B, G, T, D]
            k_attn = k_group.repeat_interleave(group_size, dim=1)
            v_attn = v_group.repeat_interleave(group_size, dim=1)
        else:
            # baseline: num_kv_groups == n_head
            k_attn = k.transpose(1, 2)
            v_attn = v.transpose(1, 2)

        # 5) Scaled dot-product attention (unchanged interface)
        y = F.scaled_dot_product_attention(
            q_attn, k_attn, v_attn,
            is_causal=True,
        )
        # 6) Re-assemble heads and project out
        y = y.transpose(1, 2).contiguous().view_as(x)
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config, layer_idx):
        super().__init__()
        self.layer_idx = layer_idx
        self.attn = CausalSelfAttention(config, layer_idx=layer_idx)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    
    # attention experiment flags
    use_mdha : bool = False
    use_gqa : bool = False
    mdha_kernel_size : int = 3
    num_kv_groups : int = 1
    num_gqa_layers : int = 0


class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config, layer_idx=i) for i in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float()
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :])
            logits = logits.float()
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 500 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end

    # attention experiment flags (wired to env vars for CLI control)
    use_mdha : bool = bool(int(os.getenv("USE_MDHA", "0")))
    use_gqa : bool = bool(int(os.getenv("USE_GQA", "0")))
    mdha_kernel_size : int = int(os.getenv("MDHA_KERNEL_SIZE", "3"))
    num_kv_groups : int = int(os.getenv("NUM_KV_GROUPS", "1"))
    num_gqa_layers : int = int(os.getenv("NUM_GQA_LAYERS", "0"))

args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(
    vocab_size=num_vocab,
    n_layer=12,
    n_head=6,
    n_embd=768,
    use_mdha=args.use_mdha,
    use_gqa=args.use_gqa,
    mdha_kernel_size=args.mdha_kernel_size,
    num_kv_groups=args.num_kv_groups,
    num_gqa_layers=args.num_gqa_layers,
))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
if master_process:
    print(f"use_mdha={args.use_mdha}, use_gqa={args.use_gqa}, "
          f"mdha_kernel_size={args.mdha_kernel_size}, "
          f"num_kv_groups={args.num_kv_groups}, num_gqa_layers={args.num_gqa_layers}")
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
# optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
#                                weight_decay=args.weight_decay, fused=True)
# optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
# optimizers = [optimizer1, optimizer2]
# collect transformer.h params into Muon-safe (2D) and the rest
muon_params = []
extra_adam_params = []

for name, p in raw_model.transformer.h.named_parameters():
    if p.ndim == 2:
        muon_params.append(p)
    else:
        extra_adam_params.append(p)

# AdamW: lm_head + any non-2D transformer params (e.g., conv weights, biases)
optimizer1 = torch.optim.AdamW(
    list(raw_model.lm_head.parameters()) + extra_adam_params,
    lr=args.learning_rate,
    betas=(0.9, 0.95),
    weight_decay=args.weight_decay,
    fused=True,
)

# Muon: only 2D parameters in the transformer blocks
optimizer2 = Muon(muon_params, lr=0.1*args.learning_rate, momentum=0.95)

optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

        

        f.write("Experiment config:\n")
        f.write(f"  use_mdha       = {args.use_mdha}\n")
        f.write(f"  mdha_kernel_sz = {args.mdha_kernel_size}\n")
        f.write(f"  use_gqa        = {args.use_gqa}\n")
        f.write(f"  num_kv_groups  = {args.num_kv_groups}\n")
        f.write(f"  num_gqa_layers = {args.num_gqa_layers}\n")
        f.write('='*100 + '\n')


training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            _, loss = model(x, y, return_logits=False)
            train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Mon Dec  8 18:40:50 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:90:00.0 Off |                    0 |
| N/A   43C    P0             90W /  400W |    2467MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:B7:00.0 Off |                    0 |
| N/A   31C    P0             85W /  400W |    2469MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Experiment config:
  use_mdha       = False
  mdha_kernel_sz = 3
  use_gqa        = True
  num_kv_groups  = 1
  num_gqa_layers = 8
====================================================================================================
step:0/500 val_loss:15.9776 train_time:230ms step_avg:nanms
step:1/500 train_loss:15.9776 train_time:88534ms step_avg:nanms
step:2/500 train_loss:9.5814 train_time:89903ms step_avg:nanms
step:3/500 train_loss:8.9191 train_time:91145ms step_avg:nanms
step:4/500 train_loss:8.5519 train_time:92382ms step_avg:nanms
step:5/500 train_loss:8.1370 train_time:93624ms step_avg:nanms
step:6/500 train_loss:7.7113 train_time:95080ms step_avg:nanms
step:7/500 train_loss:7.5272 train_time:96320ms step_avg:nanms
step:8/500 train_loss:7.0312 train_time:97563ms step_avg:nanms
step:9/500 train_loss:7.4347 train_time:98828ms step_avg:nanms
step:10/500 train_loss:7.2115 train_time:100079ms step_avg:nanms
step:11/500 train_loss:7.1244 train_time:1232ms step_avg:nanms
step:12/500 train_loss:6.9953 train_time:2483ms step_avg:nanms
step:13/500 train_loss:6.9180 train_time:3748ms step_avg:1249.23ms
step:14/500 train_loss:6.8217 train_time:4996ms step_avg:1248.99ms
step:15/500 train_loss:6.8363 train_time:6271ms step_avg:1254.22ms
step:16/500 train_loss:6.6528 train_time:7531ms step_avg:1255.09ms
step:17/500 train_loss:6.6126 train_time:8787ms step_avg:1255.26ms
step:18/500 train_loss:7.0135 train_time:10041ms step_avg:1255.09ms
step:19/500 train_loss:6.6038 train_time:11296ms step_avg:1255.08ms
step:20/500 train_loss:6.7578 train_time:12549ms step_avg:1254.89ms
step:21/500 train_loss:6.5706 train_time:13817ms step_avg:1256.10ms
step:22/500 train_loss:6.4393 train_time:15115ms step_avg:1259.56ms
step:23/500 train_loss:6.4640 train_time:16368ms step_avg:1259.09ms
step:24/500 train_loss:6.4689 train_time:17621ms step_avg:1258.63ms
step:25/500 train_loss:6.2689 train_time:18878ms step_avg:1258.54ms
step:26/500 train_loss:6.4589 train_time:20128ms step_avg:1257.98ms
step:27/500 train_loss:6.3858 train_time:21385ms step_avg:1257.92ms
step:28/500 train_loss:6.3938 train_time:22647ms step_avg:1258.17ms
step:29/500 train_loss:6.3732 train_time:23916ms step_avg:1258.75ms
step:30/500 train_loss:6.4321 train_time:25201ms step_avg:1260.04ms
step:31/500 train_loss:6.8115 train_time:26501ms step_avg:1261.96ms
step:32/500 train_loss:6.2706 train_time:27762ms step_avg:1261.91ms
step:33/500 train_loss:6.1676 train_time:29014ms step_avg:1261.46ms
step:34/500 train_loss:6.2092 train_time:30280ms step_avg:1261.66ms
step:35/500 train_loss:6.3852 train_time:31557ms step_avg:1262.27ms
step:36/500 train_loss:6.3481 train_time:32825ms step_avg:1262.49ms
step:37/500 train_loss:6.3291 train_time:34084ms step_avg:1262.37ms
step:38/500 train_loss:6.1786 train_time:35357ms step_avg:1262.75ms
step:39/500 train_loss:6.2714 train_time:36624ms step_avg:1262.90ms
step:40/500 train_loss:6.0814 train_time:37883ms step_avg:1262.75ms
step:41/500 train_loss:6.2514 train_time:39130ms step_avg:1262.27ms
step:42/500 train_loss:6.1498 train_time:40381ms step_avg:1261.90ms
step:43/500 train_loss:6.1517 train_time:41653ms step_avg:1262.21ms
step:44/500 train_loss:6.0807 train_time:42907ms step_avg:1261.97ms
step:45/500 train_loss:5.9801 train_time:44157ms step_avg:1261.64ms
step:46/500 train_loss:6.0530 train_time:45413ms step_avg:1261.47ms
step:47/500 train_loss:5.9895 train_time:46663ms step_avg:1261.15ms
step:48/500 train_loss:6.1290 train_time:47916ms step_avg:1260.94ms
step:49/500 train_loss:5.9078 train_time:49173ms step_avg:1260.84ms
step:50/500 train_loss:6.0494 train_time:50437ms step_avg:1260.92ms
step:51/500 train_loss:6.0287 train_time:51687ms step_avg:1260.67ms
step:52/500 train_loss:6.0972 train_time:52977ms step_avg:1261.35ms
step:53/500 train_loss:5.9937 train_time:54279ms step_avg:1262.31ms
step:54/500 train_loss:6.0338 train_time:55577ms step_avg:1263.12ms
step:55/500 train_loss:5.9217 train_time:56860ms step_avg:1263.55ms
step:56/500 train_loss:5.9248 train_time:58345ms step_avg:1268.37ms
step:57/500 train_loss:5.9519 train_time:59604ms step_avg:1268.17ms
step:58/500 train_loss:5.9543 train_time:60866ms step_avg:1268.04ms
step:59/500 train_loss:6.0018 train_time:62122ms step_avg:1267.81ms
step:60/500 train_loss:5.8620 train_time:63367ms step_avg:1267.33ms
step:61/500 train_loss:5.9435 train_time:64617ms step_avg:1267.00ms
step:62/500 train_loss:5.9688 train_time:65869ms step_avg:1266.71ms
step:63/500 train_loss:5.8899 train_time:67113ms step_avg:1266.29ms
step:64/500 train_loss:5.8699 train_time:68381ms step_avg:1266.31ms
step:65/500 train_loss:5.6819 train_time:69671ms step_avg:1266.74ms
step:66/500 train_loss:5.7270 train_time:70934ms step_avg:1266.67ms
step:67/500 train_loss:5.9001 train_time:72186ms step_avg:1266.42ms
step:68/500 train_loss:5.8657 train_time:73442ms step_avg:1266.24ms
step:69/500 train_loss:5.8922 train_time:74689ms step_avg:1265.91ms
step:70/500 train_loss:5.7589 train_time:75951ms step_avg:1265.85ms
step:71/500 train_loss:5.8685 train_time:77214ms step_avg:1265.81ms
step:72/500 train_loss:5.8266 train_time:78487ms step_avg:1265.92ms
step:73/500 train_loss:5.8327 train_time:79747ms step_avg:1265.83ms
step:74/500 train_loss:5.6035 train_time:80992ms step_avg:1265.49ms
step:75/500 train_loss:5.6788 train_time:82245ms step_avg:1265.31ms
step:76/500 train_loss:5.6209 train_time:83501ms step_avg:1265.16ms
step:77/500 train_loss:5.8031 train_time:84754ms step_avg:1264.99ms
step:78/500 train_loss:5.7057 train_time:86016ms step_avg:1264.95ms
step:79/500 train_loss:5.3126 train_time:87269ms step_avg:1264.77ms
step:80/500 train_loss:5.7146 train_time:88532ms step_avg:1264.74ms
step:81/500 train_loss:5.6814 train_time:89787ms step_avg:1264.61ms
step:82/500 train_loss:5.6589 train_time:91035ms step_avg:1264.37ms
step:83/500 train_loss:5.7246 train_time:92280ms step_avg:1264.11ms
step:84/500 train_loss:5.6037 train_time:93544ms step_avg:1264.11ms
step:85/500 train_loss:5.6436 train_time:94824ms step_avg:1264.32ms
step:86/500 train_loss:5.6491 train_time:96086ms step_avg:1264.29ms
step:87/500 train_loss:5.7216 train_time:97367ms step_avg:1264.51ms
step:88/500 train_loss:5.5464 train_time:98637ms step_avg:1264.58ms
step:89/500 train_loss:5.5430 train_time:99910ms step_avg:1264.68ms
step:90/500 train_loss:5.3918 train_time:101191ms step_avg:1264.89ms
step:91/500 train_loss:5.6236 train_time:102453ms step_avg:1264.85ms
step:92/500 train_loss:5.5630 train_time:103727ms step_avg:1264.97ms
step:93/500 train_loss:5.7682 train_time:104976ms step_avg:1264.77ms
step:94/500 train_loss:5.7221 train_time:106252ms step_avg:1264.90ms
step:95/500 train_loss:5.4556 train_time:107506ms step_avg:1264.78ms
step:96/500 train_loss:5.4807 train_time:108772ms step_avg:1264.80ms
step:97/500 train_loss:5.6319 train_time:110019ms step_avg:1264.58ms
step:98/500 train_loss:5.4335 train_time:111269ms step_avg:1264.42ms
step:99/500 train_loss:5.4134 train_time:112530ms step_avg:1264.38ms
step:100/500 train_loss:5.4408 train_time:113779ms step_avg:1264.21ms
step:101/500 train_loss:5.2928 train_time:115026ms step_avg:1264.02ms
step:102/500 train_loss:5.4742 train_time:116290ms step_avg:1264.02ms
step:103/500 train_loss:5.3056 train_time:117555ms step_avg:1264.03ms
step:104/500 train_loss:5.4815 train_time:118806ms step_avg:1263.89ms
step:105/500 train_loss:5.4752 train_time:120063ms step_avg:1263.82ms
step:106/500 train_loss:5.6263 train_time:121315ms step_avg:1263.70ms
step:107/500 train_loss:5.4266 train_time:122570ms step_avg:1263.61ms
step:108/500 train_loss:5.3067 train_time:123832ms step_avg:1263.59ms
step:109/500 train_loss:5.5848 train_time:125076ms step_avg:1263.40ms
step:110/500 train_loss:5.4937 train_time:126329ms step_avg:1263.29ms
step:111/500 train_loss:5.3629 train_time:127585ms step_avg:1263.22ms
step:112/500 train_loss:5.4964 train_time:128838ms step_avg:1263.12ms
step:113/500 train_loss:5.2136 train_time:130103ms step_avg:1263.13ms
step:114/500 train_loss:5.4212 train_time:131359ms step_avg:1263.06ms
step:115/500 train_loss:5.3038 train_time:132617ms step_avg:1263.02ms
step:116/500 train_loss:5.4465 train_time:133879ms step_avg:1263.01ms
step:117/500 train_loss:5.2532 train_time:135149ms step_avg:1263.07ms
step:118/500 train_loss:5.3639 train_time:136411ms step_avg:1263.06ms
step:119/500 train_loss:5.3147 train_time:137680ms step_avg:1263.12ms
step:120/500 train_loss:5.3701 train_time:138949ms step_avg:1263.17ms
step:121/500 train_loss:5.3237 train_time:140197ms step_avg:1263.04ms
step:122/500 train_loss:5.2295 train_time:141450ms step_avg:1262.95ms
step:123/500 train_loss:5.3299 train_time:142693ms step_avg:1262.77ms
step:124/500 train_loss:5.2013 train_time:143973ms step_avg:1262.92ms
step:125/500 train_loss:5.1059 train_time:145246ms step_avg:1263.01ms
step:125/500 val_loss:5.2600 train_time:145277ms step_avg:1263.28ms
step:126/500 train_loss:5.1213 train_time:146522ms step_avg:1263.12ms
step:127/500 train_loss:5.3011 train_time:147787ms step_avg:1263.14ms
step:128/500 train_loss:5.3069 train_time:149059ms step_avg:1263.21ms
step:129/500 train_loss:5.2743 train_time:150337ms step_avg:1263.33ms
step:130/500 train_loss:5.2802 train_time:151585ms step_avg:1263.21ms
step:131/500 train_loss:5.3749 train_time:152842ms step_avg:1263.16ms
step:132/500 train_loss:5.1381 train_time:154096ms step_avg:1263.08ms
step:133/500 train_loss:5.1372 train_time:155357ms step_avg:1263.06ms
step:134/500 train_loss:5.2215 train_time:156631ms step_avg:1263.16ms
step:135/500 train_loss:5.0970 train_time:157894ms step_avg:1263.15ms
step:136/500 train_loss:5.0842 train_time:159167ms step_avg:1263.23ms
step:137/500 train_loss:5.1497 train_time:160432ms step_avg:1263.25ms
step:138/500 train_loss:5.1424 train_time:161705ms step_avg:1263.32ms
step:139/500 train_loss:5.2114 train_time:162980ms step_avg:1263.41ms
step:140/500 train_loss:5.0999 train_time:164232ms step_avg:1263.32ms
step:141/500 train_loss:5.0742 train_time:165492ms step_avg:1263.30ms
step:142/500 train_loss:5.1054 train_time:166749ms step_avg:1263.25ms
step:143/500 train_loss:5.2370 train_time:168006ms step_avg:1263.20ms
step:144/500 train_loss:5.6462 train_time:169258ms step_avg:1263.12ms
step:145/500 train_loss:5.1092 train_time:170506ms step_avg:1263.01ms
step:146/500 train_loss:5.1417 train_time:171760ms step_avg:1262.94ms
step:147/500 train_loss:5.1460 train_time:173018ms step_avg:1262.90ms
step:148/500 train_loss:4.8771 train_time:174269ms step_avg:1262.82ms
step:149/500 train_loss:5.2773 train_time:175525ms step_avg:1262.77ms
step:150/500 train_loss:5.1881 train_time:176920ms step_avg:1263.72ms
step:151/500 train_loss:5.0527 train_time:178172ms step_avg:1263.63ms
step:152/500 train_loss:5.1389 train_time:179426ms step_avg:1263.56ms
step:153/500 train_loss:5.0992 train_time:180694ms step_avg:1263.60ms
step:154/500 train_loss:4.9674 train_time:181964ms step_avg:1263.64ms
step:155/500 train_loss:4.9991 train_time:183258ms step_avg:1263.85ms
step:156/500 train_loss:5.1072 train_time:184551ms step_avg:1264.05ms
step:157/500 train_loss:5.0949 train_time:185863ms step_avg:1264.38ms
step:158/500 train_loss:5.0663 train_time:187131ms step_avg:1264.40ms
step:159/500 train_loss:4.9590 train_time:188409ms step_avg:1264.49ms
step:160/500 train_loss:4.9432 train_time:189674ms step_avg:1264.50ms
step:161/500 train_loss:4.9946 train_time:190954ms step_avg:1264.60ms
step:162/500 train_loss:5.0427 train_time:192225ms step_avg:1264.64ms
step:163/500 train_loss:5.0161 train_time:193533ms step_avg:1264.92ms
step:164/500 train_loss:4.9113 train_time:194786ms step_avg:1264.84ms
step:165/500 train_loss:4.9909 train_time:196035ms step_avg:1264.75ms
step:166/500 train_loss:5.0822 train_time:197291ms step_avg:1264.69ms
step:167/500 train_loss:4.9984 train_time:198560ms step_avg:1264.71ms
step:168/500 train_loss:4.9530 train_time:199833ms step_avg:1264.76ms
step:169/500 train_loss:4.9986 train_time:201094ms step_avg:1264.74ms
step:170/500 train_loss:4.9757 train_time:202362ms step_avg:1264.76ms
step:171/500 train_loss:4.4187 train_time:203645ms step_avg:1264.88ms
step:172/500 train_loss:4.8765 train_time:204904ms step_avg:1264.84ms
step:173/500 train_loss:4.8767 train_time:206153ms step_avg:1264.74ms
step:174/500 train_loss:5.1001 train_time:207432ms step_avg:1264.83ms
step:175/500 train_loss:5.0228 train_time:208920ms step_avg:1266.18ms
step:176/500 train_loss:4.9647 train_time:210197ms step_avg:1266.25ms
step:177/500 train_loss:5.1247 train_time:211444ms step_avg:1266.13ms
step:178/500 train_loss:4.9342 train_time:212701ms step_avg:1266.08ms
step:179/500 train_loss:4.8865 train_time:213944ms step_avg:1265.94ms
step:180/500 train_loss:4.9849 train_time:215202ms step_avg:1265.90ms
step:181/500 train_loss:4.8667 train_time:216453ms step_avg:1265.81ms
step:182/500 train_loss:4.8260 train_time:217715ms step_avg:1265.79ms
step:183/500 train_loss:4.9203 train_time:218967ms step_avg:1265.71ms
step:184/500 train_loss:5.1023 train_time:220228ms step_avg:1265.68ms
step:185/500 train_loss:4.8652 train_time:221726ms step_avg:1267.01ms
step:186/500 train_loss:4.9603 train_time:224293ms step_avg:1274.39ms
step:187/500 train_loss:4.9272 train_time:226238ms step_avg:1278.18ms
step:188/500 train_loss:4.9218 train_time:228036ms step_avg:1281.10ms
step:189/500 train_loss:4.5751 train_time:229511ms step_avg:1282.18ms
step:190/500 train_loss:4.7957 train_time:230836ms step_avg:1282.42ms
step:191/500 train_loss:4.8162 train_time:232896ms step_avg:1286.72ms
step:192/500 train_loss:4.7459 train_time:234955ms step_avg:1290.96ms
step:193/500 train_loss:4.9436 train_time:237321ms step_avg:1296.84ms
step:194/500 train_loss:4.8836 train_time:238712ms step_avg:1297.35ms
step:195/500 train_loss:5.0699 train_time:240482ms step_avg:1299.90ms
step:196/500 train_loss:4.9555 train_time:241829ms step_avg:1300.16ms
step:197/500 train_loss:4.7981 train_time:243100ms step_avg:1300.00ms
step:198/500 train_loss:4.8395 train_time:244362ms step_avg:1299.80ms
step:199/500 train_loss:4.7298 train_time:246195ms step_avg:1302.62ms
step:200/500 train_loss:4.8179 train_time:248217ms step_avg:1306.40ms
step:201/500 train_loss:4.7299 train_time:249726ms step_avg:1307.46ms
step:202/500 train_loss:4.9687 train_time:251094ms step_avg:1307.78ms
step:203/500 train_loss:4.8567 train_time:252904ms step_avg:1310.39ms
step:204/500 train_loss:4.8393 train_time:254386ms step_avg:1311.27ms
step:205/500 train_loss:4.9946 train_time:255992ms step_avg:1312.78ms
step:206/500 train_loss:4.6665 train_time:257529ms step_avg:1313.92ms
step:207/500 train_loss:4.8204 train_time:259319ms step_avg:1316.34ms
step:208/500 train_loss:4.7671 train_time:261207ms step_avg:1319.23ms
step:209/500 train_loss:4.9343 train_time:262470ms step_avg:1318.94ms
step:210/500 train_loss:4.8691 train_time:263739ms step_avg:1318.70ms
step:211/500 train_loss:4.7452 train_time:264995ms step_avg:1318.38ms
step:212/500 train_loss:4.8669 train_time:266240ms step_avg:1318.02ms
step:213/500 train_loss:4.7148 train_time:267518ms step_avg:1317.82ms
step:214/500 train_loss:4.7992 train_time:268801ms step_avg:1317.65ms
step:215/500 train_loss:4.6679 train_time:270079ms step_avg:1317.46ms
step:216/500 train_loss:4.7756 train_time:271340ms step_avg:1317.18ms
step:217/500 train_loss:4.7642 train_time:272608ms step_avg:1316.95ms
step:218/500 train_loss:4.7401 train_time:273868ms step_avg:1316.67ms
step:219/500 train_loss:4.7543 train_time:275202ms step_avg:1316.75ms
step:220/500 train_loss:4.7766 train_time:276469ms step_avg:1316.52ms
step:221/500 train_loss:4.8136 train_time:277745ms step_avg:1316.33ms
step:222/500 train_loss:4.7566 train_time:279000ms step_avg:1316.04ms
step:223/500 train_loss:4.7550 train_time:280279ms step_avg:1315.86ms
step:224/500 train_loss:4.8911 train_time:281534ms step_avg:1315.58ms
step:225/500 train_loss:4.6181 train_time:282799ms step_avg:1315.34ms
step:226/500 train_loss:4.6587 train_time:284056ms step_avg:1315.07ms
step:227/500 train_loss:4.6522 train_time:285339ms step_avg:1314.93ms
step:228/500 train_loss:4.8099 train_time:286589ms step_avg:1314.63ms
step:229/500 train_loss:4.6382 train_time:287848ms step_avg:1314.37ms
step:230/500 train_loss:4.7871 train_time:289107ms step_avg:1314.12ms
step:231/500 train_loss:4.6509 train_time:290387ms step_avg:1313.97ms
step:232/500 train_loss:4.6170 train_time:291665ms step_avg:1313.81ms
step:233/500 train_loss:4.8170 train_time:292939ms step_avg:1313.63ms
step:234/500 train_loss:4.6607 train_time:294201ms step_avg:1313.40ms
step:235/500 train_loss:4.6062 train_time:295484ms step_avg:1313.26ms
step:236/500 train_loss:4.8475 train_time:296746ms step_avg:1313.04ms
step:237/500 train_loss:4.7283 train_time:298006ms step_avg:1312.80ms
step:238/500 train_loss:4.6488 train_time:299265ms step_avg:1312.57ms
step:239/500 train_loss:4.7831 train_time:300548ms step_avg:1312.43ms
step:240/500 train_loss:4.7735 train_time:301814ms step_avg:1312.23ms
step:241/500 train_loss:4.6710 train_time:303122ms step_avg:1312.22ms
step:242/500 train_loss:4.8234 train_time:304410ms step_avg:1312.11ms
step:243/500 train_loss:4.6625 train_time:305664ms step_avg:1311.86ms
step:244/500 train_loss:4.6780 train_time:307097ms step_avg:1312.38ms
step:245/500 train_loss:4.7478 train_time:308359ms step_avg:1312.17ms
step:246/500 train_loss:4.7024 train_time:309653ms step_avg:1312.09ms
step:247/500 train_loss:4.6658 train_time:310936ms step_avg:1311.97ms
step:248/500 train_loss:4.8256 train_time:312217ms step_avg:1311.84ms
step:249/500 train_loss:4.5665 train_time:313503ms step_avg:1311.73ms
step:250/500 train_loss:4.6019 train_time:314757ms step_avg:1311.49ms
step:250/500 val_loss:4.6651 train_time:314795ms step_avg:1311.65ms
step:251/500 train_loss:4.7275 train_time:316038ms step_avg:1311.36ms
step:252/500 train_loss:4.7306 train_time:317321ms step_avg:1311.24ms
step:253/500 train_loss:4.6076 train_time:318593ms step_avg:1311.08ms
step:254/500 train_loss:4.6174 train_time:319868ms step_avg:1310.93ms
step:255/500 train_loss:4.7510 train_time:321146ms step_avg:1310.80ms
step:256/500 train_loss:4.7040 train_time:322435ms step_avg:1310.71ms
step:257/500 train_loss:4.6763 train_time:323703ms step_avg:1310.54ms
step:258/500 train_loss:4.6035 train_time:324984ms step_avg:1310.42ms
step:259/500 train_loss:4.6224 train_time:326266ms step_avg:1310.31ms
step:260/500 train_loss:4.6882 train_time:329018ms step_avg:1316.07ms
step:261/500 train_loss:4.6976 train_time:333824ms step_avg:1329.98ms
step:262/500 train_loss:4.6063 train_time:335091ms step_avg:1329.73ms
step:263/500 train_loss:4.5497 train_time:336379ms step_avg:1329.56ms
step:264/500 train_loss:4.6086 train_time:337710ms step_avg:1329.56ms
step:265/500 train_loss:4.4701 train_time:339023ms step_avg:1329.50ms
step:266/500 train_loss:4.5202 train_time:340693ms step_avg:1330.83ms
step:267/500 train_loss:4.5605 train_time:341960ms step_avg:1330.58ms
step:268/500 train_loss:4.5166 train_time:343213ms step_avg:1330.28ms
step:269/500 train_loss:4.4846 train_time:344500ms step_avg:1330.12ms
step:270/500 train_loss:4.7099 train_time:347057ms step_avg:1334.83ms
step:271/500 train_loss:4.6373 train_time:348327ms step_avg:1334.59ms
step:272/500 train_loss:4.5024 train_time:351047ms step_avg:1339.87ms
step:273/500 train_loss:4.5508 train_time:352625ms step_avg:1340.78ms
step:274/500 train_loss:4.6684 train_time:355566ms step_avg:1346.84ms
step:275/500 train_loss:4.6799 train_time:356916ms step_avg:1346.85ms
step:276/500 train_loss:4.8694 train_time:358343ms step_avg:1347.15ms
step:277/500 train_loss:4.6259 train_time:368483ms step_avg:1380.08ms
step:278/500 train_loss:4.7533 train_time:369915ms step_avg:1380.28ms
step:279/500 train_loss:4.6012 train_time:371216ms step_avg:1379.99ms
step:280/500 train_loss:4.6702 train_time:372644ms step_avg:1380.16ms
step:281/500 train_loss:4.5605 train_time:374539ms step_avg:1382.06ms
step:282/500 train_loss:4.6950 train_time:376189ms step_avg:1383.05ms
step:283/500 train_loss:4.5005 train_time:378896ms step_avg:1387.90ms
step:284/500 train_loss:4.6687 train_time:380539ms step_avg:1388.83ms
step:285/500 train_loss:4.6518 train_time:381825ms step_avg:1388.45ms
step:286/500 train_loss:4.6853 train_time:383186ms step_avg:1388.35ms
step:287/500 train_loss:4.5455 train_time:384448ms step_avg:1387.90ms
step:288/500 train_loss:4.6090 train_time:385750ms step_avg:1387.59ms
step:289/500 train_loss:4.4726 train_time:387212ms step_avg:1387.86ms
step:290/500 train_loss:4.4724 train_time:388913ms step_avg:1388.98ms
step:291/500 train_loss:4.6063 train_time:390161ms step_avg:1388.47ms
step:292/500 train_loss:4.4826 train_time:391430ms step_avg:1388.05ms
step:293/500 train_loss:4.5282 train_time:392720ms step_avg:1387.70ms
step:294/500 train_loss:4.5496 train_time:393972ms step_avg:1387.22ms
step:295/500 train_loss:4.4206 train_time:395256ms step_avg:1386.86ms
step:296/500 train_loss:4.4084 train_time:396557ms step_avg:1386.56ms
step:297/500 train_loss:4.4425 train_time:397833ms step_avg:1386.18ms
step:298/500 train_loss:4.5497 train_time:399095ms step_avg:1385.75ms
step:299/500 train_loss:4.4332 train_time:400355ms step_avg:1385.31ms
step:300/500 train_loss:4.6066 train_time:401617ms step_avg:1384.89ms
step:301/500 train_loss:4.5776 train_time:402899ms step_avg:1384.53ms
step:302/500 train_loss:4.4996 train_time:404161ms step_avg:1384.11ms
step:303/500 train_loss:4.5633 train_time:405433ms step_avg:1383.73ms
step:304/500 train_loss:4.5527 train_time:406685ms step_avg:1383.28ms
step:305/500 train_loss:5.0267 train_time:407943ms step_avg:1382.86ms
step:306/500 train_loss:4.5105 train_time:409200ms step_avg:1382.43ms
step:307/500 train_loss:4.4124 train_time:410499ms step_avg:1382.15ms
step:308/500 train_loss:4.6044 train_time:411782ms step_avg:1381.82ms
step:309/500 train_loss:4.3945 train_time:413055ms step_avg:1381.45ms
step:310/500 train_loss:4.6383 train_time:414308ms step_avg:1381.03ms
step:311/500 train_loss:4.5382 train_time:415582ms step_avg:1380.67ms
step:312/500 train_loss:4.4531 train_time:416860ms step_avg:1380.33ms
step:313/500 train_loss:4.5850 train_time:418165ms step_avg:1380.08ms
step:314/500 train_loss:4.7064 train_time:419473ms step_avg:1379.84ms
step:315/500 train_loss:4.5480 train_time:420720ms step_avg:1379.41ms
step:316/500 train_loss:4.4294 train_time:421991ms step_avg:1379.06ms
step:317/500 train_loss:4.4490 train_time:423250ms step_avg:1378.67ms
step:318/500 train_loss:4.4679 train_time:424515ms step_avg:1378.30ms
step:319/500 train_loss:4.4190 train_time:425789ms step_avg:1377.96ms
step:320/500 train_loss:4.5173 train_time:427096ms step_avg:1377.73ms
step:321/500 train_loss:4.5251 train_time:428357ms step_avg:1377.35ms
step:322/500 train_loss:4.4826 train_time:429634ms step_avg:1377.03ms
step:323/500 train_loss:4.5519 train_time:430913ms step_avg:1376.72ms
step:324/500 train_loss:4.5503 train_time:432172ms step_avg:1376.34ms
step:325/500 train_loss:4.6132 train_time:433428ms step_avg:1375.96ms
step:326/500 train_loss:4.4655 train_time:434710ms step_avg:1375.66ms
step:327/500 train_loss:4.9105 train_time:435966ms step_avg:1375.29ms
step:328/500 train_loss:4.6206 train_time:437217ms step_avg:1374.90ms
step:329/500 train_loss:4.4222 train_time:438494ms step_avg:1374.59ms
step:330/500 train_loss:4.3802 train_time:439744ms step_avg:1374.20ms
step:331/500 train_loss:4.5275 train_time:440999ms step_avg:1373.83ms
step:332/500 train_loss:4.4325 train_time:442262ms step_avg:1373.48ms
step:333/500 train_loss:4.4344 train_time:443536ms step_avg:1373.18ms
step:334/500 train_loss:4.3955 train_time:444809ms step_avg:1372.87ms
step:335/500 train_loss:4.5782 train_time:446059ms step_avg:1372.49ms
step:336/500 train_loss:4.5267 train_time:447306ms step_avg:1372.10ms
step:337/500 train_loss:5.0474 train_time:448564ms step_avg:1371.75ms
step:338/500 train_loss:4.4940 train_time:449820ms step_avg:1371.40ms
step:339/500 train_loss:4.4648 train_time:451107ms step_avg:1371.15ms
step:340/500 train_loss:4.4516 train_time:452383ms step_avg:1370.86ms
step:341/500 train_loss:4.3861 train_time:453638ms step_avg:1370.51ms
step:342/500 train_loss:4.3608 train_time:454924ms step_avg:1370.25ms
step:343/500 train_loss:4.4248 train_time:456173ms step_avg:1369.89ms
step:344/500 train_loss:4.5271 train_time:457431ms step_avg:1369.55ms
step:345/500 train_loss:4.4172 train_time:458675ms step_avg:1369.18ms
step:346/500 train_loss:4.3468 train_time:459929ms step_avg:1368.84ms
step:347/500 train_loss:4.3981 train_time:461174ms step_avg:1368.47ms
step:348/500 train_loss:4.4060 train_time:462422ms step_avg:1368.11ms
step:349/500 train_loss:4.3383 train_time:463670ms step_avg:1367.76ms
step:350/500 train_loss:4.0219 train_time:464912ms step_avg:1367.39ms
step:351/500 train_loss:4.3123 train_time:466169ms step_avg:1367.06ms
step:352/500 train_loss:4.6637 train_time:467429ms step_avg:1366.75ms
step:353/500 train_loss:4.1934 train_time:468696ms step_avg:1366.46ms
step:354/500 train_loss:4.4490 train_time:469955ms step_avg:1366.15ms
step:355/500 train_loss:4.3409 train_time:471199ms step_avg:1365.79ms
step:356/500 train_loss:4.4418 train_time:472471ms step_avg:1365.52ms
step:357/500 train_loss:4.4355 train_time:473721ms step_avg:1365.19ms
step:358/500 train_loss:4.3601 train_time:474992ms step_avg:1364.92ms
step:359/500 train_loss:4.6286 train_time:476244ms step_avg:1364.60ms
step:360/500 train_loss:4.1032 train_time:477499ms step_avg:1364.28ms
step:361/500 train_loss:4.5690 train_time:478761ms step_avg:1363.99ms
step:362/500 train_loss:4.4714 train_time:480010ms step_avg:1363.66ms
step:363/500 train_loss:4.3582 train_time:481257ms step_avg:1363.33ms
step:364/500 train_loss:4.2940 train_time:482533ms step_avg:1363.09ms
step:365/500 train_loss:4.4476 train_time:483784ms step_avg:1362.77ms
step:366/500 train_loss:4.3875 train_time:485048ms step_avg:1362.50ms
step:367/500 train_loss:4.3681 train_time:486333ms step_avg:1362.28ms
step:368/500 train_loss:4.3697 train_time:487577ms step_avg:1361.95ms
step:369/500 train_loss:4.2591 train_time:488831ms step_avg:1361.65ms
step:370/500 train_loss:4.4126 train_time:490083ms step_avg:1361.34ms
step:371/500 train_loss:4.3413 train_time:491358ms step_avg:1361.10ms
step:372/500 train_loss:4.2254 train_time:492657ms step_avg:1360.93ms
step:373/500 train_loss:4.4165 train_time:493943ms step_avg:1360.72ms
step:374/500 train_loss:4.3496 train_time:495187ms step_avg:1360.40ms
step:375/500 train_loss:4.3446 train_time:496441ms step_avg:1360.11ms
step:375/500 val_loss:4.3617 train_time:496484ms step_avg:1360.23ms
step:376/500 train_loss:4.4038 train_time:497725ms step_avg:1359.90ms
step:377/500 train_loss:4.2953 train_time:498980ms step_avg:1359.62ms
step:378/500 train_loss:4.3476 train_time:500243ms step_avg:1359.36ms
step:379/500 train_loss:4.4252 train_time:501492ms step_avg:1359.06ms
step:380/500 train_loss:4.4492 train_time:502744ms step_avg:1358.77ms
step:381/500 train_loss:4.2599 train_time:506326ms step_avg:1364.76ms
step:382/500 train_loss:4.3246 train_time:507596ms step_avg:1364.51ms
step:383/500 train_loss:4.2838 train_time:508876ms step_avg:1364.28ms
step:384/500 train_loss:4.3853 train_time:510137ms step_avg:1364.00ms
step:385/500 train_loss:4.1897 train_time:511409ms step_avg:1363.76ms
step:386/500 train_loss:4.4029 train_time:512655ms step_avg:1363.45ms
step:387/500 train_loss:4.2805 train_time:513906ms step_avg:1363.14ms
step:388/500 train_loss:4.4515 train_time:515166ms step_avg:1362.87ms
step:389/500 train_loss:4.3402 train_time:516420ms step_avg:1362.59ms
step:390/500 train_loss:4.3818 train_time:517674ms step_avg:1362.30ms
step:391/500 train_loss:4.2709 train_time:518953ms step_avg:1362.08ms
step:392/500 train_loss:4.3553 train_time:520221ms step_avg:1361.84ms
step:393/500 train_loss:4.3073 train_time:521481ms step_avg:1361.57ms
step:394/500 train_loss:4.3278 train_time:522744ms step_avg:1361.31ms
step:395/500 train_loss:4.3086 train_time:524020ms step_avg:1361.09ms
step:396/500 train_loss:4.3060 train_time:525288ms step_avg:1360.85ms
step:397/500 train_loss:4.1003 train_time:526567ms step_avg:1360.64ms
step:398/500 train_loss:4.3179 train_time:527818ms step_avg:1360.36ms
step:399/500 train_loss:4.3073 train_time:529090ms step_avg:1360.13ms
step:400/500 train_loss:4.2553 train_time:530339ms step_avg:1359.84ms
step:401/500 train_loss:4.3643 train_time:531600ms step_avg:1359.59ms
step:402/500 train_loss:4.2474 train_time:532849ms step_avg:1359.31ms
step:403/500 train_loss:4.5118 train_time:534138ms step_avg:1359.13ms
step:404/500 train_loss:4.4246 train_time:535392ms step_avg:1358.86ms
step:405/500 train_loss:4.3607 train_time:536639ms step_avg:1358.58ms
step:406/500 train_loss:4.3831 train_time:537889ms step_avg:1358.31ms
step:407/500 train_loss:4.3097 train_time:539136ms step_avg:1358.02ms
step:408/500 train_loss:4.2440 train_time:540390ms step_avg:1357.76ms
step:409/500 train_loss:4.3352 train_time:541631ms step_avg:1357.47ms
step:410/500 train_loss:4.2630 train_time:542907ms step_avg:1357.27ms
step:411/500 train_loss:4.3175 train_time:544181ms step_avg:1357.06ms
step:412/500 train_loss:4.2963 train_time:545436ms step_avg:1356.81ms
step:413/500 train_loss:4.2821 train_time:546691ms step_avg:1356.55ms
step:414/500 train_loss:4.3707 train_time:547945ms step_avg:1356.30ms
step:415/500 train_loss:4.2504 train_time:549199ms step_avg:1356.05ms
step:416/500 train_loss:4.2838 train_time:550442ms step_avg:1355.77ms
step:417/500 train_loss:4.3976 train_time:551700ms step_avg:1355.53ms
step:418/500 train_loss:4.1619 train_time:552948ms step_avg:1355.26ms
step:419/500 train_loss:4.4499 train_time:554228ms step_avg:1355.08ms
step:420/500 train_loss:4.4965 train_time:555482ms step_avg:1354.83ms
step:421/500 train_loss:4.2965 train_time:556730ms step_avg:1354.57ms
step:422/500 train_loss:4.4089 train_time:558000ms step_avg:1354.37ms
step:423/500 train_loss:4.1434 train_time:559247ms step_avg:1354.11ms
step:424/500 train_loss:4.2651 train_time:560496ms step_avg:1353.86ms
step:425/500 train_loss:4.1728 train_time:561744ms step_avg:1353.60ms
step:426/500 train_loss:4.3702 train_time:562997ms step_avg:1353.36ms
step:427/500 train_loss:4.3465 train_time:564247ms step_avg:1353.11ms
step:428/500 train_loss:4.2500 train_time:565498ms step_avg:1352.87ms
step:429/500 train_loss:4.3587 train_time:566750ms step_avg:1352.63ms
step:430/500 train_loss:4.2205 train_time:568004ms step_avg:1352.39ms
step:431/500 train_loss:4.1584 train_time:569250ms step_avg:1352.14ms
step:432/500 train_loss:4.3579 train_time:570517ms step_avg:1351.94ms
step:433/500 train_loss:4.3816 train_time:571768ms step_avg:1351.70ms
step:434/500 train_loss:4.3584 train_time:573022ms step_avg:1351.47ms
step:435/500 train_loss:4.2483 train_time:574274ms step_avg:1351.23ms
step:436/500 train_loss:4.3452 train_time:575519ms step_avg:1350.98ms
step:437/500 train_loss:4.3919 train_time:576779ms step_avg:1350.77ms
step:438/500 train_loss:4.3144 train_time:578024ms step_avg:1350.52ms
step:439/500 train_loss:4.3620 train_time:579282ms step_avg:1350.31ms
step:440/500 train_loss:4.2133 train_time:580528ms step_avg:1350.06ms
step:441/500 train_loss:4.2952 train_time:581772ms step_avg:1349.82ms
step:442/500 train_loss:4.2717 train_time:583018ms step_avg:1349.58ms
step:443/500 train_loss:4.1408 train_time:584266ms step_avg:1349.34ms
step:444/500 train_loss:4.3015 train_time:585509ms step_avg:1349.10ms
step:445/500 train_loss:4.5055 train_time:586774ms step_avg:1348.90ms
step:446/500 train_loss:4.1673 train_time:588026ms step_avg:1348.68ms
step:447/500 train_loss:4.3511 train_time:589275ms step_avg:1348.46ms
step:448/500 train_loss:4.3429 train_time:590525ms step_avg:1348.23ms
step:449/500 train_loss:4.2018 train_time:591793ms step_avg:1348.05ms
step:450/500 train_loss:4.2151 train_time:593037ms step_avg:1347.81ms
step:451/500 train_loss:4.2566 train_time:594287ms step_avg:1347.59ms
step:452/500 train_loss:4.5658 train_time:595532ms step_avg:1347.36ms
step:453/500 train_loss:4.4457 train_time:596793ms step_avg:1347.16ms
step:454/500 train_loss:4.2700 train_time:598043ms step_avg:1346.94ms
step:455/500 train_loss:4.2075 train_time:599320ms step_avg:1346.79ms
step:456/500 train_loss:4.3291 train_time:600579ms step_avg:1346.59ms
step:457/500 train_loss:4.3054 train_time:601832ms step_avg:1346.38ms
step:458/500 train_loss:4.2953 train_time:603076ms step_avg:1346.15ms
step:459/500 train_loss:4.3605 train_time:604326ms step_avg:1345.94ms
step:460/500 train_loss:4.2292 train_time:605606ms step_avg:1345.79ms
step:461/500 train_loss:4.3174 train_time:606868ms step_avg:1345.61ms
step:462/500 train_loss:4.2475 train_time:608138ms step_avg:1345.44ms
step:463/500 train_loss:4.1003 train_time:609415ms step_avg:1345.29ms
step:464/500 train_loss:4.2519 train_time:610669ms step_avg:1345.09ms
step:465/500 train_loss:4.3560 train_time:611920ms step_avg:1344.88ms
step:466/500 train_loss:4.2228 train_time:613205ms step_avg:1344.75ms
step:467/500 train_loss:4.2650 train_time:614461ms step_avg:1344.55ms
step:468/500 train_loss:4.2254 train_time:615710ms step_avg:1344.35ms
step:469/500 train_loss:4.4284 train_time:616957ms step_avg:1344.13ms
step:470/500 train_loss:4.2955 train_time:618230ms step_avg:1343.98ms
step:471/500 train_loss:4.1241 train_time:619482ms step_avg:1343.78ms
step:472/500 train_loss:4.3198 train_time:620726ms step_avg:1343.56ms
step:473/500 train_loss:4.2109 train_time:621982ms step_avg:1343.37ms
step:474/500 train_loss:4.3245 train_time:623239ms step_avg:1343.19ms
step:475/500 train_loss:4.3609 train_time:624510ms step_avg:1343.03ms
step:476/500 train_loss:4.5287 train_time:625768ms step_avg:1342.85ms
step:477/500 train_loss:4.3308 train_time:627032ms step_avg:1342.68ms
step:478/500 train_loss:4.3328 train_time:628285ms step_avg:1342.49ms
step:479/500 train_loss:4.2459 train_time:629527ms step_avg:1342.28ms
step:480/500 train_loss:4.1847 train_time:630785ms step_avg:1342.10ms
step:481/500 train_loss:4.2652 train_time:632033ms step_avg:1341.90ms
step:482/500 train_loss:4.3824 train_time:633300ms step_avg:1341.74ms
step:483/500 train_loss:4.2713 train_time:634566ms step_avg:1341.58ms
step:484/500 train_loss:4.3270 train_time:635817ms step_avg:1341.39ms
step:485/500 train_loss:4.2987 train_time:637082ms step_avg:1341.22ms
step:486/500 train_loss:4.1243 train_time:638333ms step_avg:1341.04ms
step:487/500 train_loss:4.2883 train_time:639603ms step_avg:1340.89ms
step:488/500 train_loss:4.2769 train_time:640870ms step_avg:1340.73ms
step:489/500 train_loss:4.2543 train_time:642140ms step_avg:1340.58ms
step:490/500 train_loss:4.4597 train_time:643416ms step_avg:1340.45ms
step:491/500 train_loss:4.2551 train_time:644691ms step_avg:1340.31ms
step:492/500 train_loss:4.1850 train_time:645934ms step_avg:1340.11ms
step:493/500 train_loss:4.3258 train_time:647189ms step_avg:1339.94ms
step:494/500 train_loss:4.0966 train_time:648449ms step_avg:1339.77ms
step:495/500 train_loss:4.2758 train_time:649722ms step_avg:1339.63ms
step:496/500 train_loss:4.4498 train_time:651003ms step_avg:1339.51ms
step:497/500 train_loss:4.2618 train_time:652254ms step_avg:1339.33ms
step:498/500 train_loss:4.2315 train_time:653516ms step_avg:1339.17ms
step:499/500 train_loss:4.2182 train_time:654783ms step_avg:1339.02ms
step:500/500 train_loss:4.3417 train_time:656036ms step_avg:1338.85ms
step:500/500 val_loss:4.2556 train_time:656073ms step_avg:1338.92ms
